{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "30457eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import shap\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ParameterSampler, train_test_split, GridSearchCV, cross_val_score, StratifiedKFold, RandomizedSearchCV, GroupShuffleSplit, StratifiedGroupKFold\n",
    "from sklearn.metrics import precision_recall_curve, classification_report, accuracy_score, f1_score, make_scorer, log_loss, recall_score, precision_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import randint, uniform\n",
    "import xgboost as xgb\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "# Neural Network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6116cb9",
   "metadata": {},
   "source": [
    "## Define Statics ## \n",
    "\n",
    "- Columns of dataset containing metadata\n",
    "- Feature Columns\n",
    "- Reduced Features after removing high correlations\n",
    "- Average values over training set for missing values\n",
    "- Integer encoding of label-valued features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "044f1511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns that are relevant for filtering and data splitting\n",
    "cols = ['MatchId',\n",
    "        'SUBTYPE',\n",
    "        'EventId',\n",
    "        'CUID1',\n",
    "        'Club1',\n",
    "        'possesion_chain',\n",
    "        'possesion_chain_team',\n",
    "        'possesion_chain_team_id',\n",
    "        'spielphase',\n",
    "        'subphase',\n",
    "        'time_in_sec_sync',\n",
    "        'possesion_chain_defending_team']\n",
    "\n",
    "cols_match_id = ['MatchId']\n",
    "\n",
    "cols_epv_target = ['epv_success']\n",
    "\n",
    "cols_kpis = [\n",
    "    'sp_type',\n",
    "    'sp_initial_position_x_distance_to_goal',\n",
    "    'sp_initial_position_y_distance_to_goal',\n",
    "    'sp_initial_position_distance_to_goal',\n",
    "    'sp_initial_position_angle_to_goal',\n",
    "    'sp_initial_position_horizontal_zone',\n",
    "    'sp_initial_position_vertical_zone',\n",
    "    'sp_initial_last_line_of_defense',\n",
    "    'sp_initial_inswing_outswing',\n",
    "    'sp_initial_marking_type',\n",
    "    'sp_first_ball_action_type',\n",
    "    'sp_first_ball_action_execution',\n",
    "    'sp_first_ball_action_ball_z_max',\n",
    "    'sp_delivery_time',\n",
    "    'sp_delivery_distance',\n",
    "    'sp_delivery_velocity',\n",
    "    'sp_delivery_end_position_horizontal_zone',\n",
    "    'sp_delivery_end_position_vertical_zone',\n",
    "    'sp_delivery_end_position_distance_to_goal',\n",
    "    'sp_delivery_end_position_angle_to_goal',\n",
    "    'sp_delivery_in_golden_zone',\n",
    "    'sp_action',\n",
    "    'sp_pass_number',\n",
    "    'sp_duration',\n",
    "    'sp_position_distance_to_goal',\n",
    "    'sp_position_angle_to_goal',\n",
    "    'sp_position_horizontal_zone',\n",
    "    'sp_position_vertical_zone',\n",
    "    'sp_numerical_superiority_in_box_offense',\n",
    "    'sp_numerical_superiority_in_dfb_shot_zone_offense',\n",
    "    'sp_space_control_in_box_offense',\n",
    "    'sp_space_control_in_dfb_shot_zone_offense',\n",
    "    'sp_heights_in_box_offense',\n",
    "    'sp_heights_shot_zone_offense',\n",
    "    'sp_avg_speeds_box_offense',\n",
    "    'sp_avg_speeds_shot_zone_offense',\n",
    "    'sp_defenders_in_line_of_shot',\n",
    "    'sp_heights_in_box_defense',\n",
    "    'sp_heights_shot_zone_defense',\n",
    "    'sp_defensive_pressure_box',\n",
    "    'sp_defensive_pressure_shot_zone',\n",
    "    'sp_early_dropping'\n",
    "]\n",
    "\n",
    "all_cols = cols + cols_epv_target + cols_kpis\n",
    "\n",
    "cols_kpis_final = [\n",
    "    # Category I: Initial State\n",
    "    'sp_type',\n",
    "    'sp_initial_position_x_distance_to_goal',\n",
    "    'sp_initial_position_y_distance_to_goal',\n",
    "    'sp_initial_position_distance_to_goal',\n",
    "    'sp_initial_position_angle_to_goal',\n",
    "    'sp_initial_position_horizontal_zone',\n",
    "    'sp_initial_position_vertical_zone',\n",
    "    'sp_initial_last_line_of_defense',\n",
    "    'sp_initial_inswing_outswing',\n",
    "    'sp_initial_marking_type',\n",
    "\n",
    "    # Category II: Initial Exectution\n",
    "    'sp_first_ball_action_type',\n",
    "    'sp_first_ball_action_execution',\n",
    "    'sp_first_ball_action_ball_z_max',\n",
    "    'sp_delivery_time',\n",
    "    'sp_delivery_distance',\n",
    "    'sp_delivery_velocity',\n",
    "    'sp_delivery_end_position_horizontal_zone',\n",
    "    'sp_delivery_end_position_vertical_zone',\n",
    "    'sp_delivery_end_position_distance_to_goal',\n",
    "    'sp_delivery_end_position_angle_to_goal',\n",
    "    'sp_delivery_in_golden_zone',\n",
    "\n",
    "    # Category III: Game State after Execution\n",
    "    # 'sp_action',\n",
    "    'sp_pass_number',\n",
    "    'sp_duration',\n",
    "    'sp_position_distance_to_goal',\n",
    "    'sp_position_angle_to_goal',\n",
    "    'sp_position_horizontal_zone',\n",
    "    'sp_position_vertical_zone',\n",
    "\n",
    "    # Category IV: Attacking Team Performance\n",
    "    'sp_numerical_superiority_in_box_offense',\n",
    "    'sp_numerical_superiority_in_dfb_shot_zone_offense',\n",
    "    'sp_space_control_in_box_offense',\n",
    "    'sp_space_control_in_dfb_shot_zone_offense',\n",
    "    # 'sp_heights_in_box_offense',\n",
    "    'sp_heights_in_box_offense_min',  # nachberechnet\n",
    "    'sp_heights_in_box_offense_max',  # nachberechnet\n",
    "    'sp_heights_in_box_offense_mean',  # nachberechnet\n",
    "    'sp_heights_in_box_offense_count',  # nachberechnet\n",
    "    'sp_heights_in_box_offense_taller_than_defense',  # nachberechnet\n",
    "    # 'sp_heights_shot_zone_offense',\n",
    "    'sp_heights_shot_zone_offense_min',  # nachberechnet\n",
    "    'sp_heights_shot_zone_offense_max',  # nachberechnet\n",
    "    'sp_heights_shot_zone_offense_mean',  # nachberechnet\n",
    "    'sp_heights_shot_zone_offense_count',  # nachberechnet\n",
    "    'sp_heights_shot_zone_offense_taller_than_defense',  # nachberechnet\n",
    "    # 'sp_avg_speeds_box_offense',\n",
    "    'sp_avg_speeds_box_offense_min',  # nachberechnet\n",
    "    'sp_avg_speeds_box_offense_max',  # nachberechnet\n",
    "    'sp_avg_speeds_box_offense_mean',  # nachberechnet\n",
    "    # 'sp_avg_speeds_shot_zone_offense',\n",
    "    'sp_avg_speeds_shot_zone_offense_min',  # nachberechnet\n",
    "    'sp_avg_speeds_shot_zone_offense_max',  # nachberechnet\n",
    "    'sp_avg_speeds_shot_zone_offense_mean',  # nachberechnet\n",
    "\n",
    "\n",
    "    # Category V: Defending Team Performance\n",
    "    'sp_defenders_in_line_of_shot',\n",
    "    # 'sp_heights_in_box_defense',\n",
    "    'sp_heights_in_box_defense_min',  # nachberechnet\n",
    "    'sp_heights_in_box_defense_max',  # nachberechnet\n",
    "    'sp_heights_in_box_defense_mean',  # nachberechnet\n",
    "    # 'sp_heights_shot_zone_defense',\n",
    "    'sp_heights_shot_zone_defense_min',  # nachberechnet\n",
    "    'sp_heights_shot_zone_defense_max',  # nachberechnet\n",
    "    'sp_heights_shot_zone_defense_mean',  # nachberechnet\n",
    "    # 'sp_defensive_pressure_box',\n",
    "    'sp_defensive_pressure_box_min',  # nachberechnet\n",
    "    'sp_defensive_pressure_box_max',  # nachberechnet\n",
    "    'sp_defensive_pressure_box_mean',  # nachberechnet\n",
    "    # 'sp_defensive_pressure_shot_zone',\n",
    "    'sp_defensive_pressure_shot_zone_min',  # nachberechnet\n",
    "    'sp_defensive_pressure_shot_zone_max',  # nachberechnet\n",
    "    'sp_defensive_pressure_shot_zone_mean',  # nachberechnet\n",
    "    'sp_early_dropping'\n",
    "]\n",
    "\n",
    "col_nonnumerical = [\n",
    "    'sp_type',\n",
    "    'sp_initial_inswing_outswing',\n",
    "    'sp_initial_marking_type',\n",
    "    'sp_first_ball_action_type',\n",
    "    'sp_first_ball_action_execution',\n",
    "    'sp_action'\n",
    "]\n",
    "\n",
    "cols_heights = [\n",
    "    'sp_heights_in_box_offense',\n",
    "    'sp_heights_shot_zone_offense',\n",
    "    'sp_heights_in_box_defense',\n",
    "    'sp_heights_shot_zone_defense'\n",
    "]\n",
    "\n",
    "cols_speeds = [\n",
    "    'sp_avg_speeds_box_offense',\n",
    "    'sp_avg_speeds_shot_zone_offense'\n",
    "]\n",
    "\n",
    "cols_def_pressure = [\n",
    "    'sp_defensive_pressure_box',\n",
    "    'sp_defensive_pressure_shot_zone'\n",
    "]\n",
    "\n",
    "# hochkorrelierende Spalten Eckb√§lle\n",
    "cols_with_high_correlation = [\n",
    "    'sp_initial_position_y_distance_to_goal',\n",
    "    'sp_initial_position_x_distance_to_goal',\n",
    "    # 'sp_initial_last_line_of_defense',\n",
    "    'sp_initial_position_vertical_zone',\n",
    "    'sp_initial_position_horizontal_zone',\n",
    "    'sp_initial_position_distance_to_goal',\n",
    "    'sp_initial_position_angle_to_goal',\n",
    "    # 'sp_delivery_end_position_vertical_zone',\n",
    "    'sp_delivery_end_position_horizontal_zone',\n",
    "    # 'sp_delivery_end_position_distance_to_goal',\n",
    "    'sp_delivery_in_golden_zone',\n",
    "    'sp_first_ball_action_execution',\n",
    "    'sp_first_ball_action_type',\n",
    "    # 'sp_position_vertical_zone',\n",
    "    'sp_heights_in_box_offense_count',\n",
    "    'sp_heights_in_box_defense_min',\n",
    "    'sp_heights_in_box_defense_max',\n",
    "    'sp_heights_in_box_defense_mean',\n",
    "    # 'sp_heights_in_box_offense_count',\n",
    "    'sp_heights_in_box_offense_min',\n",
    "    'sp_heights_in_box_offense_max',\n",
    "    'sp_heights_in_box_offense_mean',\n",
    "    #\n",
    "    'sp_heights_in_box_offense_taller_than_defense',\n",
    "    # 'sp_heights_shot_zone_offense_min',\n",
    "    # 'sp_heights_shot_zone_offense_max', #probiere stattdessen ohne mean aus?\n",
    "    # 'sp_heights_shot_zone_offense_count',\n",
    "    'sp_heights_shot_zone_defense_min',\n",
    "    'sp_numerical_superiority_in_box_offense',\n",
    "    # 'sp_numerical_superiority_in_dfb_shot_zone_offense',\n",
    "    # 'sp_space_control_in_box_offense',\n",
    "    'sp_defensive_pressure_box_max',\n",
    "    'sp_defensive_pressure_box_min',\n",
    "    'sp_defensive_pressure_box_mean',\n",
    "    'sp_defensive_pressure_shot_zone_max',\n",
    "    # 'sp_defensive_pressure_shot_zone_mean',\n",
    "    'sp_avg_speeds_box_offense_mean',\n",
    "    'sp_avg_speeds_box_offense_max',\n",
    "    'sp_avg_speeds_box_offense_min',\n",
    "    # 'sp_avg_speeds_shot_zone_offense_mean'\n",
    "    'sp_avg_speeds_shot_zone_offense_max',\n",
    "]\n",
    "\n",
    "# Entferne hoch-korrelierende Spalten aus kpis f√ºr Modell\n",
    "cols_kpis_final_no_corr = [col for col in cols_kpis_final if col not in cols_with_high_correlation]\n",
    "\n",
    "all_cols_final = cols + cols_epv_target + cols_kpis_final\n",
    "\n",
    "set_pieces = ['Eckball', 'Freisto√ü', 'Einwurf', 'Elfmeter', 'Spieler√∂ffnung']\n",
    "\n",
    "# Durchschnittswerte f√ºr height cols statt nan values\n",
    "avg_heights_dict = {\n",
    "    'sp_heights_in_box_offense_max': 190.8,\n",
    "    'sp_heights_in_box_offense_min': 179.5,\n",
    "    'sp_heights_in_box_offense_mean': 185.6,\n",
    "    'sp_heights_shot_zone_offense_max': 190.4,\n",
    "    'sp_heights_shot_zone_offense_min': 180.6,\n",
    "    'sp_heights_shot_zone_offense_mean': 185.9,\n",
    "    'sp_heights_in_box_defense_max': 192.9,\n",
    "    'sp_heights_in_box_defense_min': 183.4,\n",
    "    'sp_heights_in_box_defense_mean': 188.7,\n",
    "    'sp_heights_shot_zone_defense_max': 192.8,\n",
    "    'sp_heights_shot_zone_defense_min': 184.3,\n",
    "    'sp_heights_shot_zone_defense_mean': 188.9\n",
    "}\n",
    "\n",
    "avg_speeds_dict = {\n",
    "    'sp_avg_speeds_box_offense_max': 3.3,\n",
    "    'sp_avg_speeds_box_offense_min': 1.2,\n",
    "    'sp_avg_speeds_box_offense_mean': 2.1,\n",
    "    'sp_avg_speeds_shot_zone_offense_max': 3.0,\n",
    "    'sp_avg_speeds_shot_zone_offense_min': 1.2,\n",
    "    'sp_avg_speeds_shot_zone_offense_mean': 2.0\n",
    "}\n",
    "\n",
    "avg_def_pressure_dict = {\n",
    "    'sp_defensive_pressure_box_max': 66.5,\n",
    "    'sp_defensive_pressure_box_min': 19.6,\n",
    "    'sp_defensive_pressure_box_mean': 44.2,\n",
    "    'sp_defensive_pressure_shot_zone_max': 65.2,\n",
    "    'sp_defensive_pressure_shot_zone_min': 25.6,\n",
    "    'sp_defensive_pressure_shot_zone_mean': 46.6\n",
    "}\n",
    "\n",
    "encoding_dict = {\n",
    "    'sp_type': {\n",
    "        'Eckball': 1,\n",
    "        'Freisto√ü': 2,\n",
    "        'Einwurf': 3,\n",
    "        'Elfmeter': 4\n",
    "    },\n",
    "    'sp_initial_inswing_outswing': {\n",
    "        'inswing': -1,\n",
    "        'neutral': 0,\n",
    "        'outswing': 1\n",
    "    },\n",
    "    'sp_initial_marking_type': {\n",
    "        'zone_marking': -1,\n",
    "        'mixed': 0,\n",
    "        'man_marking': 1,\n",
    "        'unknown': 2\n",
    "    },\n",
    "    'sp_first_ball_action_type': {\n",
    "        'Pass': 1,\n",
    "        'Cross': 2,\n",
    "        'ShotAtGoal': 3\n",
    "    },\n",
    "    'sp_first_ball_action_execution': {\n",
    "        'high': 1,\n",
    "        'flat': 2\n",
    "    },\n",
    "    'sp_action': {\n",
    "        'Pass': 1,\n",
    "        'Cross': 2,\n",
    "        'ShotAtGoal': 3,\n",
    "        'OtherBallAction': 4,\n",
    "        'BallClaiming': 5,\n",
    "        'TacklingGame': 6\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58ce5c7",
   "metadata": {},
   "source": [
    "## Functions for Model Training, Selection and Visualization ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_chains_containing_subphase(events, subphase):\n",
    "    \n",
    "#     event_chains_to_be_removed = events[\n",
    "#         events['subphase'] == subphase\n",
    "#         ][['MatchId', 'possesion_chain']].drop_duplicates()\n",
    "\n",
    "#     print(f'Entferne {len(event_chains_to_be_removed)} possesion-chains, die mit Spieler√∂ffnung beginnen')\n",
    "\n",
    "#     # Entferne alle Chains, die 'Spieler√∂ffnung' als subphase haben\n",
    "#     # for chain in event_chains_to_be_removed:\n",
    "#     #     events_season = events_season[events_season[['MatchId','possesion_chain']] != chain]\n",
    "#     events = events.merge(\n",
    "#         event_chains_to_be_removed,\n",
    "#         on=['MatchId', 'possesion_chain'],\n",
    "#         how='left',\n",
    "#         indicator=True\n",
    "#     ).query(\"_merge == 'left_only'\").drop(columns=['_merge'])\n",
    "#     print(f\"{len(events)} Events √ºbrig\")\n",
    "#     return events\n",
    "\n",
    "\n",
    "# def transform_height_column(events, col):\n",
    "#     # instead of nan values, use mean value for the variable\n",
    "#     events[col+'_max'] = events[col].apply(lambda x: max(x) if len(x) > 0 else avg_heights_dict[col+'_max'])\n",
    "#     events[col+'_min'] = events[col].apply(lambda x: min(x) if len(x) > 0 else avg_heights_dict[col+'_min'])\n",
    "#     events[col+'_mean'] = events[col].apply(lambda x: np.mean(x) if len(x) > 0 else avg_heights_dict[col+'_mean'])\n",
    "#     events[col+'_count'] = events[col].apply(lambda x: len(x))\n",
    "\n",
    "# def transform_speed_column(events, col):\n",
    "#     events[col+'_max'] = events[col].apply(lambda x: max(x) if len(x) > 0 else avg_speeds_dict[col+'_max'])\n",
    "#     events[col+'_min'] = events[col].apply(lambda x: min(x) if len(x) > 0 else avg_speeds_dict[col+'_min'])\n",
    "#     events[col+'_mean'] = events[col].apply(lambda x: np.mean(x) if len(x) > 0 else avg_speeds_dict[col+'_mean'])\n",
    "\n",
    "# def transform_def_pressure_column(events, col):\n",
    "#     events[col+'_max'] = events[col].apply(lambda x: max(x) if len(x) > 0 else avg_def_pressure_dict[col+'_max'])\n",
    "#     events[col+'_min'] = events[col].apply(lambda x: min(x) if len(x) > 0 else avg_def_pressure_dict[col+'_min'])\n",
    "#     events[col+'_mean'] = events[col].apply(lambda x: np.mean(x) if len(x) > 0 else avg_def_pressure_dict[col+'_mean'])\n",
    "\n",
    "# def transform_list_columns(events):\n",
    "#     # Transformiere List columns zu einzelnen Spalten\n",
    "#     # 1. z√§hle Angreifer, die gr√∂√üer sind als der gr√∂√üte Verteidiger\n",
    "#     events['sp_heights_in_box_offense_taller_than_defense'] = 0\n",
    "#     events['sp_heights_shot_zone_offense_taller_than_defense'] = 0\n",
    "#     for i, row in events.iterrows():\n",
    "\n",
    "#         if len(row['sp_heights_in_box_defense']) > 1:\n",
    "#             # take second highest height\n",
    "#             max_def_height = sorted(row['sp_heights_in_box_defense'])[-2]\n",
    "#         elif len(row['sp_heights_in_box_defense']) == 1:\n",
    "#             max_def_height = row['sp_heights_in_box_defense'][0]\n",
    "#         else:\n",
    "#             max_def_height = 0\n",
    "#         events.loc[i, 'sp_heights_in_box_offense_taller_than_defense'] = len(\n",
    "#             [h for h in row['sp_heights_in_box_offense'] if h > max_def_height])\n",
    "\n",
    "#         if len(row['sp_heights_shot_zone_defense']) > 1:\n",
    "#             # take second highest height\n",
    "#             max_def_height = sorted(row['sp_heights_shot_zone_defense'])[-2]\n",
    "#         elif len(row['sp_heights_shot_zone_defense']) == 1:\n",
    "#             max_def_height = row['sp_heights_shot_zone_defense'][0]\n",
    "#         else:\n",
    "#             max_def_height = 0\n",
    "#         events.loc[i, 'sp_heights_shot_zone_offense_taller_than_defense'] = len(\n",
    "#             [h for h in row['sp_heights_shot_zone_offense'] if h > max_def_height])\n",
    "\n",
    "#     # 2. berechne min, max, mean\n",
    "#     for col in cols_heights:\n",
    "#         transform_height_column(events, col)\n",
    "#     for col in cols_speeds:\n",
    "#         transform_speed_column(events, col)\n",
    "#     for col in cols_def_pressure:\n",
    "#         transform_def_pressure_column(events, col)\n",
    "\n",
    "#     return events\n",
    "\n",
    "# def transform_nonnumerical_columns(events):\n",
    "#     for col in col_nonnumerical:\n",
    "#         events[col] = events[col].map(encoding_dict[col])\n",
    "#     return events\n",
    "\n",
    "### correlation\n",
    "def print_high_correlations(df, threshold=0.75):\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    # Dict for sorting\n",
    "    corr_dict = {}\n",
    "\n",
    "    # Fill dict with correlations above threshold\n",
    "    for col1 in corr_matrix.columns:\n",
    "        for col2 in corr_matrix.columns:\n",
    "            correlation = corr_matrix.loc[col1, col2]\n",
    "            if col1 != col2 and abs(correlation) > threshold:\n",
    "                corr_dict[f\"{col1} - {col2}\"] = correlation\n",
    "                # print(f\"{col1} - {col2}: {correlation:.2f}\")\n",
    "\n",
    "    # Sort dict\n",
    "    sorted_corr_dict = dict(sorted(corr_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    if len(sorted_corr_dict) == 0:\n",
    "        print(f\"No correlations above {threshold}\")\n",
    "    else:\n",
    "        print(f\"Sorted correlations above {threshold}:\")\n",
    "        for key, value in sorted_corr_dict.items():\n",
    "            print(f\"{key}: {value:.2f}\")\n",
    "\n",
    "# def perform_grid_search_rf(X_train, y_train, X_test, y_test):\n",
    "\n",
    "#     # Definiere Grid\n",
    "#     param_grid = {\"max_features\": [\"log2\"], #[\"sqrt\", \"log2\"],\n",
    "#                   \"max_depth\": range(12, 22, 2)  # range(5, 20, 2)\n",
    "#                   }\n",
    "#     param_grid[\"min_samples_split\"] = [2,3]#[2, 3, 5, 7]  # [2,4,6,8,10]\n",
    "#     param_grid[\"min_samples_leaf\"] = [11,12,13,14]#[6,7,8,9,11,13,15] # [2, 4, 7, 9]  # [1,2,4,6,8,10]\n",
    "#     param_grid[\"class_weight\"] = [{0: 1, 1:50}]# [\"balanced_subsample\", \"balanced\", {0: 1, 1: 30}, {0: 1, 1: 50}]# [None, \"balanced_subsample\", \"balanced\"]\n",
    "#     param_grid[\"n_estimators\"] = range(50, 80, 5) # range (20, 80, 10) # number of trees in the random forest\n",
    "#     param_grid[\"bootstrap\"] = [True]  # method used to sample data points\n",
    "#     param_grid[\"criterion\"] = [\"gini\"]#[\"gini\", \"entropy\"] # function to measure the quality\n",
    "\n",
    "#     # print grid size:\n",
    "#     print(f\"Grid size: {len(param_grid['max_features']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf']) * len(param_grid['class_weight']) * len(param_grid['n_estimators']) * len(param_grid['bootstrap']) * len(param_grid['criterion'])}\")\n",
    "\n",
    "#     # Definiere Scoring\n",
    "#     scoring = \"f1\" # \"recall\"\n",
    "\n",
    "#     # benuzte f1 scorer mit average='binary' und pos_label=1 (misst f1 score der minority class)\n",
    "#     f1_scorer = make_scorer(f1_score, average='binary', pos_label=1)\n",
    "\n",
    "#     # benutze stratified k-fold cross validation, sodass die Klassenverteilung in den folds gleich bleibt\n",
    "#     cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "#     # Angepasste Grid Search\n",
    "#     rf = RandomForestClassifier(random_state=42)\n",
    "#     grid_search = GridSearchCV(\n",
    "#         estimator=rf,\n",
    "#         param_grid=param_grid,\n",
    "#         scoring=f1_scorer,  # Use custom F1 scorer\n",
    "#         cv=cv,\n",
    "#         n_jobs=-1,  # Use all available processors\n",
    "#         verbose=0\n",
    "#     )\n",
    "\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     # Beste Parameter aus der Grid Search\n",
    "#     print(\"Best Parameters from f1:\", grid_search.best_params_)\n",
    "\n",
    "#     # Trainiere bestes Modell auf gesamten Trainingsdaten\n",
    "#     best_rf = grid_search.best_estimator_\n",
    "\n",
    "#     # Perform Cross-Validation\n",
    "#     scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring=scoring)\n",
    "\n",
    "#     print(f\"Cross-Validation Scores {scoring} with training data:\", scores)\n",
    "\n",
    "#     best_rf.fit(X_train, y_train)\n",
    "\n",
    "#     # Predict\n",
    "#     y_pred = best_rf.predict(X_test)\n",
    "\n",
    "#     # Bewerte das Modell\n",
    "#     print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "#     print(\"Classification Report:\\n\", classification_report(y_train, y_pred))\n",
    "\n",
    "\n",
    "#     n_majority = len(y_train) - sum(y_train)\n",
    "#     n_minority = sum(y_train)\n",
    "\n",
    "#     # logge Grid Search und Ergebnisse\n",
    "#     filename = 'src/s02_database/utils/logfile_grid_search_rf.txt'\n",
    "#     with open(filename, 'a') as file:\n",
    "#         file.write(f\"Date: {datetime.datetime.now()}\\n\")\n",
    "#         file.write(f\"Sample Numbers majority/minority class: Proportion {n_majority/n_minority} {n_majority}/{n_minority}\\n\")\n",
    "#         file.write(f\"Grid Search with Scoring: {scoring}\\n\")\n",
    "#         file.write(f\"Parameters: {param_grid}\\n\")\n",
    "#         file.write(f\"Best Parameters: {grid_search.best_params_}\\n\")\n",
    "#         file.write(f\"Accuracy: {accuracy_score(y_test, y_pred)}\\n\")\n",
    "#         file.write(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\\n\")\n",
    "#         file.write(\"\\n\")\n",
    "\n",
    "#     # Gebe bestes Modell zur√ºck\n",
    "#     return best_rf\n",
    "\n",
    "# def perform_randomized_search_rf(X_train, y_train, n = 61, selection = 'alle', seed = 42):\n",
    "\n",
    "#         # Definiere Suchbereich\n",
    "#         param_dist = {\n",
    "#             \"max_features\": [\"sqrt\", \"log2\"],  # [\"sqrt\", \"log2\"],\n",
    "#             \"max_depth\": randint(2, 7),  # range(5, 20, 2)\n",
    "#             \"min_samples_split\": randint(2, 9),  # [2, 3, 5, 7]  # [2,4,6,8,10]\n",
    "#             \"min_samples_leaf\": randint(2, 9),\n",
    "#             \"class_weight\": [\"balanced\", \"balanced_subsample\", {0: 1, 1: 50}, {0: 1, 1: 70}],\n",
    "#             # [\"balanced_subsample\", \"balanced\",{0: 1, 1:50}], #[\"balanced\", \"balanced_subsample\", {0: 1, 1:50}],# [\"balanced_subsample\", \"balanced\", {0: 1, 1: 30}, {0: 1, 1: 50}]# [None, \"balanced_subsample\", \"balanced\"]\n",
    "#             \"n_estimators\": randint(20, 50),  # range (20, 80, 10) # number of trees in the random forest\n",
    "#             \"bootstrap\": [False, True],  # [True, False],  # method used to sample data points\n",
    "#             \"criterion\": [\"entropy\"]  # [\"gini\", \"entropy\"] # function to measure the quality\n",
    "#         }\n",
    "\n",
    "#         # Definiere Scoring\n",
    "#         scoring = \"f1\"  # \"recall\"\n",
    "\n",
    "#         # benuzte f1 scorer mit average='binary' und pos_label=1 (misst f1 score der minority class)\n",
    "#         f1_scorer = make_scorer(f1_score, average='binary', pos_label=1)\n",
    "\n",
    "#         # benutze stratified k-fold cross validation, sodass die Klassenverteilung in den folds gleich bleibt\n",
    "#         cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "#         # Angepasste Randomized Search\n",
    "#         rf = RandomForestClassifier(random_state=seed)\n",
    "#         randomized_search = RandomizedSearchCV(\n",
    "#             estimator=rf,\n",
    "#             param_distributions=param_dist,\n",
    "#             n_iter=50,\n",
    "#             scoring=f1_scorer,  # Use custom F1 scorer\n",
    "#             cv=cv,\n",
    "#             n_jobs=-1,  # Use all available processors\n",
    "#             verbose=1\n",
    "#         )\n",
    "\n",
    "#         randomized_search.fit(X_train, y_train)\n",
    "#         # Beste Scores und Parameter aus der Grid Search\n",
    "#         print(\"Best Score from f1:\", randomized_search.best_score_)\n",
    "#         # Beste CV Scores\n",
    "#         # print(\"Best CV Scores from f1:\", randomized_search.cv_results_['mean_test_score'])\n",
    "#         print(\"Best Parameters from f1:\", randomized_search.best_params_)\n",
    "\n",
    "\n",
    "#         # Perform Cross-Validation\n",
    "#         # scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring=scoring)\n",
    "\n",
    "#         n_majority = len(y_train) - sum(y_train)\n",
    "#         n_minority = sum(y_train)\n",
    "\n",
    "#         # logge Grid Search und Ergebnisse\n",
    "#         filename = 'logfile_grid_search_rf.txt'\n",
    "#         with open(filename, 'a') as file:\n",
    "#             file.write(f\"Date: {datetime.datetime.now()}\\n\")\n",
    "#             file.write(f\"Seed: {seed}\\n\")\n",
    "#             file.write(\n",
    "#                 f\"Sample Numbers majority/minority class: Proportion {n_majority / n_minority} {n_majority}/{n_minority}\\n\")\n",
    "#             file.write(f\"{n} Features selected by selection {selection}\\n\")\n",
    "#             file.write(f\"Randomized Search with Scoring: {scoring}\\n\")\n",
    "#             file.write(f\"Parameters: {param_dist}\\n\")\n",
    "#             file.write(f\"Best Parameters: {randomized_search.best_params_}\\n\")\n",
    "#             file.write(f\"Best Score: {randomized_search.best_score_}\\n\")\n",
    "#             file.write(\"\\n\")\n",
    "#         return randomized_search.best_params_, randomized_search.best_score_, randomized_search.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def smote_oversampling(X_train, y_train, proportion=1/3):\n",
    "#     # Balance classes with SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "#     desired_proportion = proportion\n",
    "\n",
    "#     if \"MatchId\" in X_train.columns:\n",
    "#         X_train = X_train.drop(columns=[\"MatchId\"])\n",
    "\n",
    "#     # Z√§hle Anzahl der Samples der Klasse 0 und 1\n",
    "#     n_majority = len(y_train) - sum(y_train)\n",
    "#     n_minority = sum(y_train)\n",
    "\n",
    "#     print(\"Anzahl Samples der Klasse 1 vor SMOTE:\", n_minority)\n",
    "#     print(\"Anzahl Samples der Klasse 0 vor SMOTE:\", n_majority)\n",
    "\n",
    "#     # Resample zum gew√ºnschten Verh√§ltnis (upsampling minority class)\n",
    "#     sampling_strategy = {0: int(n_majority), 1: int(n_majority * desired_proportion)}\n",
    "#     smote = SMOTE(random_state=42, sampling_strategy=sampling_strategy)\n",
    "#     X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "#     # X_test_smote, y_test_smote = smote.fit_resample(X_test, y_test)\n",
    "\n",
    "#     n_majority = len(y_train_smote) - sum(y_train_smote)\n",
    "#     n_minority = sum(y_train_smote)\n",
    "\n",
    "#     print(\"Anzahl Samples der Klasse 1 nach SMOTE:\", n_minority)\n",
    "#     print(\"Anzahl Samples der Klasse 0 nach SMOTE:\", n_majority)\n",
    "\n",
    "#     return X_train_smote, y_train_smote\n",
    "\n",
    "def show_feature_importance(rf_classifier, X):\n",
    "    # # Feature Importance\n",
    "    # Get feature importances\n",
    "    feature_names = X.columns\n",
    "    importances = rf_classifier.feature_importances_\n",
    "\n",
    "    # Create a DataFrame for better visualization\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    print(feature_importance_df)\n",
    "\n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
    "    plt.title('Feature Importances')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance Score')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def return_top_n_features_wo_high_cor(rf_classifier, X, threshold=0.7, n = 20):\n",
    "    # Sortiere Features nach importance und w√§hle top 15, die nicht miteinander korrelieren\n",
    "    features = X.columns\n",
    "    importances = rf_classifier.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # sortiere features nach importance\n",
    "    features = features[indices]\n",
    "    # Korrelationsmatrix\n",
    "    corr = X.corr()\n",
    "    # sortiere reihen und spalten nach importances\n",
    "    corr = corr.iloc[indices, indices]\n",
    "    top_fifteen_ft_wo_corr = []\n",
    "    while len(top_fifteen_ft_wo_corr) < n:\n",
    "        for i in range(len(indices)):\n",
    "            if i == 0:\n",
    "                top_fifteen_ft_wo_corr.append(features[i])\n",
    "            else:\n",
    "                append_flag = True\n",
    "                for j in range(i):\n",
    "                    if abs(corr.iloc[i, j]) > threshold:\n",
    "                        append_flag = False\n",
    "                if append_flag and len(top_fifteen_ft_wo_corr) < n:\n",
    "                    top_fifteen_ft_wo_corr.append(features[i])\n",
    "    return top_fifteen_ft_wo_corr\n",
    "\n",
    "def train_and_save_rf(X_train, y_train, X_test, y_test, filename, params):\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(random_state=42)\n",
    "    rf_classifier.set_params(**params)\n",
    "\n",
    "    # Perform Cross-Validation\n",
    "    # scores = cross_val_score(rf_classifier, X_train, y_train, cv=5, scoring=scoring)\n",
    "    scores = cross_val_score(rf_classifier, X_train, y_train,\n",
    "                             cv=StratifiedKFold(n_splits=5), scoring='f1')\n",
    "    print(f\"Cross-Validation Scores {scoring}:\", scores)\n",
    "\n",
    "    # Train\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # SAVE MODEL\n",
    "    # save the model to disk\n",
    "    filename = f'models/{filename}.pkl'\n",
    "    joblib.dump(rf_classifier, filename)\n",
    "\n",
    "def train_best_rf_all_features(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    best_params = {'bootstrap': False, 'class_weight': {0: 1, 1: 50}, 'criterion': 'entropy', 'max_depth': 26,\n",
    "                   'max_features': 'log2', 'min_samples_leaf': 15, 'min_samples_split': 14, 'n_estimators': 46}\n",
    "    # Cross-Validation Scores f1 with training data: [0.51674641 0.37209302 0.42342342 0.37073171 0.42592593]\n",
    "    # Accuracy: 0.9739854318418314\n",
    "    # Classification Report:\n",
    "    #                precision    recall  f1-score   support\n",
    "    #\n",
    "    #          0.0       0.98      0.99      0.99      4648\n",
    "    #          1.0       0.65      0.45      0.53       157\n",
    "    #\n",
    "    #     accuracy                           0.97      4805\n",
    "    #    macro avg       0.81      0.72      0.76      4805\n",
    "    # weighted avg       0.97      0.97      0.97      4805\n",
    "\n",
    "    train_and_save_rf(X_train, y_train, X_test, y_test, 'epv_setpiece_allFeatures_randomForrest', best_params)\n",
    "\n",
    "def train_best_rf_selected_features(X_train, y_train, X_test, y_test, n, selection):\n",
    "\n",
    "    # Best Parameters from f1:\n",
    "    best_params = {'bootstrap': False, 'class_weight': {0: 1, 1: 60}, 'criterion': 'entropy', 'max_depth': 34,\n",
    "                   'max_features': 'log2', 'min_samples_leaf': 10, 'min_samples_split': 9, 'n_estimators': 64}\n",
    "    # Cross-Validation Scores f1 with training data: [0.53403141 0.36649215 0.39408867 0.29834254 0.41450777]\n",
    "    # Accuracy: 0.9741935483870968\n",
    "    # Classification Report:\n",
    "    #                precision    recall  f1-score   support\n",
    "    #\n",
    "    #          0.0       0.98      0.99      0.99      4648\n",
    "    #          1.0       0.70      0.36      0.48       157\n",
    "    #\n",
    "    #     accuracy                           0.97      4805\n",
    "    #    macro avg       0.84      0.68      0.73      4805\n",
    "    # weighted avg       0.97      0.97      0.97      4805\n",
    "\n",
    "    train_and_save_rf(X_train, y_train, X_test, y_test, f'epv_setpiece_top{n}_{selection}_randomForrest', best_params)\n",
    "\n",
    "def split_data_into_n_folds(X, y, random_state=42, n_splits = 5):\n",
    "\n",
    "    strat_group_kfold = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    X_train_splits = []\n",
    "    X_test_splits = []\n",
    "    y_train_splits = []\n",
    "    y_test_splits = []\n",
    "    for train_idx, test_idx in strat_group_kfold.split(X, y, groups=X['MatchId']):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        X_train_splits.append(X_train)\n",
    "        X_test_splits.append(X_test)\n",
    "        y_train_splits.append(y_train)\n",
    "        y_test_splits.append(y_test)\n",
    "\n",
    "    return X_train_splits, X_test_splits, y_train_splits, y_test_splits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a custom weighted F1 scorer\n",
    "def weighted_f1_score(y_true, y_pred):\n",
    "    # Calculate F1-scores for both classes\n",
    "    f1_class_0 = f1_score(y_true, y_pred, pos_label=0)\n",
    "    f1_class_1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    roc = roc_auc_score(y_true,y_pred)\n",
    "    log_loss = log_loss(y_true, y_pred)\n",
    "\n",
    "    # # Enforce conditions (accuracy > 0.7 and recall > 0.1)\n",
    "    # if accuracy < 0.7 or recall < 0.1:\n",
    "    #     result = 0  # Penalize heavily if conditions are not met\n",
    "    # else:\n",
    "    #     result = 0.2 * recall + 0.8 * accuracy\n",
    "\n",
    "    # # Custom ROC Scorer, der niedrige recalls und accuracy bestraft\n",
    "    result = roc * min(recall/0.20,accuracy/0.9,1)\n",
    "\n",
    "    return result\n",
    "\n",
    "def perform_randomized_search_nested_cv_rf(X, y, random_state=42):\n",
    "\n",
    "    # Definiere Suchbereich\n",
    "    param_dist = {\n",
    "        \"max_features\": [\"sqrt\"],  # [\"sqrt\", \"log2\"],\n",
    "        \"max_depth\": randint(8, 15),  # range(5, 20, 2)\n",
    "        \"min_samples_split\": randint(2, 4),  # [2, 3, 5, 7]  # [2,4,6,8,10]\n",
    "        \"min_samples_leaf\": randint(2, 4),\n",
    "        \"class_weight\": [None,\"balanced\",\"balanced_subsample\"],\n",
    "        # [\"balanced_subsample\", \"balanced\",{0: 1, 1:50}], #[\"balanced\", \"balanced_subsample\", {0: 1, 1:50}],# [\"balanced_subsample\", \"balanced\", {0: 1, 1: 30}, {0: 1, 1: 50}]# [None, \"balanced_subsample\", \"balanced\"]\n",
    "        \"n_estimators\": randint(10, 40),  # range (20, 80, 10) # number of trees in the random forest\n",
    "        \"bootstrap\": [True],  # [True, False],  # method used to sample data points\n",
    "        \"criterion\": [\"gini\"]  # [\"gini\", \"entropy\"] # function to measure the quality\n",
    "    }\n",
    "\n",
    "    # benuzte f1 scorer mit average='binary' und pos_label=1 (misst f1 score der minority class)\n",
    "\n",
    "    # scorer = make_scorer(f1_score, pos_label=1)\n",
    "    # scorer = make_scorer(f1_score, average='macro')\n",
    "    # scorer = make_scorer(weighted_f1_score)\n",
    "    # scorer = make_scorer(roc_auc_score)\n",
    "    scorer = make_scorer(log_loss)\n",
    "    # Erstelle Folds\n",
    "    n_outer = 4\n",
    "    n_inner = 3\n",
    "    n_fts = len(X.columns) - 1\n",
    "\n",
    "    #if file \"df_training_results.pkl\" exists, load it\n",
    "    file_name_results = \"models/df_training_results.pkl\"\n",
    "    file_name_importances = \"models/df_ft_imp.pkl\"\n",
    "    file_name_hyper_params = \"models/df_hyper_params.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "    if os.path.isfile(file_name_results):\n",
    "        df_model_kpis = pd.read_pickle(file_name_results)\n",
    "        score_to_beat = df_model_kpis['test_score'].max()\n",
    "\n",
    "    else:\n",
    "        df_model_kpis = pd.DataFrame(\n",
    "            columns=['seed', 'n_outer', 'scoring', 'cv_training_score', 'test_score','f1', 'accuracy', 'recall', 'precision', 'roc_auc', 'confusion_matrix'])\n",
    "        score_to_beat = 0\n",
    "    if os.path.isfile(file_name_importances):\n",
    "        df_ft_imp = pd.read_pickle(file_name_importances)\n",
    "    else:\n",
    "        df_ft_imp = pd.DataFrame(columns=['seed', 'n_outer'] + list(X.columns))\n",
    "\n",
    "    if os.path.isfile(file_name_hyper_params):\n",
    "        df_hyper_params = pd.read_pickle(file_name_hyper_params)\n",
    "    else:\n",
    "        df_hyper_params = pd.DataFrame(columns=['seed', 'n_outer', 'max_features', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'class_weight', 'n_estimators', 'bootstrap', 'criterion'])\n",
    "\n",
    "    outer_X_train_folds, outer_X_test_folds, outer_y_train_folds, outer_y_test_folds = split_data_into_n_folds(X, y, random_state=random_state, n_splits=n_outer)\n",
    "\n",
    "    for i in range(n_outer):\n",
    "        X_train = outer_X_train_folds[i]\n",
    "        X_test = outer_X_test_folds[i]\n",
    "        y_train = outer_y_train_folds[i]\n",
    "        y_test = outer_y_test_folds[i]\n",
    "\n",
    "        # quadruple minority class\n",
    "        index = y_train[y_train == 1].index\n",
    "        X_train = pd.concat([X_train, X_train.loc[index]], ignore_index=True)\n",
    "        y_train = pd.concat([y_train, y_train.loc[index]], ignore_index=True)\n",
    "        index = y_train[y_train == 1].index\n",
    "        X_train = pd.concat([X_train, X_train.loc[index]], ignore_index=True)\n",
    "        y_train = pd.concat([y_train, y_train.loc[index]], ignore_index=True)\n",
    "\n",
    "        print(f\"{i+1}. Outer Fold: Train/Test Split:\", len(y_train) - sum(y_train), sum(y_train))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Perform Randomized Search with inner Cross Validation\n",
    "        rf = RandomForestClassifier(random_state=random_state)\n",
    "        randomized_search = RandomizedSearchCV(\n",
    "            estimator=rf,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=100,#100\n",
    "            scoring=scorer,#f1_scorer,  # Use custom F1 scorer\n",
    "            n_jobs=-1,  # Use all available processors\n",
    "            verbose=1,\n",
    "            cv=fold\n",
    "        )\n",
    "\n",
    "        randomized_search.fit(X_train, y_train, groups=groups)\n",
    "\n",
    "        # Beste Scores und Parameter aus der Grid Search\n",
    "        print(f\"{i+1}. Outer Fold: Best CV-Score from Hyperparameter-Search:\", round(randomized_search.best_score_,2))\n",
    "        # print(f\"{i+1}. Outer Fold: Best Parameters from f1:\", randomized_search.best_params_)\n",
    "\n",
    "        #predict test data\n",
    "        y_pred = randomized_search.predict(X_test)\n",
    "        # score = f1_score(y_test, y_pred, average='binary', pos_label=1)\n",
    "        print(f\"{i+1}. Outer Fold Test Score from f1:\",round(weighted_f1_score(y_test, y_pred),2))\n",
    "        print(f\"{i+1}. Outer Fold Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "        # print(f\"{i+1}. Outer Fold Recall Score:\", round(recall_score(y_test, y_pred),2))\n",
    "        # print(f\"{i+1}. Outer Fold Precision Score:\", round(precision_score(y_test, y_pred), 2))\n",
    "        # print(f\"{i+1}. Outer Fold F1 Score:\", round(f1_score(y_test, y_pred),2))\n",
    "        # print(f\"{i+1}. Outer Fold Accuracy Score:\", round(accuracy_score(y_test, y_pred),2))\n",
    "        print(f\"{i+1}. Outer Fold Area under ROC Curve:\", round(roc_auc_score(y_test, y_pred), 2))\n",
    "        # print(f\"{i+1}. Outer Fold Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "        # if score is the best so far, save model\n",
    "        if weighted_f1_score(y_test, y_pred) > score_to_beat:\n",
    "            print(\"New best score!\")\n",
    "            score_to_beat = weighted_f1_score(y_test, y_pred)\n",
    "            # save model\n",
    "            filename = f'models/epv_setpiece_randomForrest_best{n_fts}fts.pkl'\n",
    "            joblib.dump(randomized_search, filename)\n",
    "            print(f\"{i+1}. Outer Fold: Model saved\")\n",
    "\n",
    "\n",
    "        # top 10 features\n",
    "        importances = randomized_search.best_estimator_.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        features = X_train.columns\n",
    "        features = features[indices]\n",
    "        # print(f\"{i+1}. Outer Fold: Top 10 Features:\")\n",
    "        # for j in range(10):\n",
    "        #     print(features[j] + \":\", round(100 * importances[indices[j]]), \"%\")\n",
    "\n",
    "        #fill row of df_model_kpis, df_ft_imp and df_hyper_params\n",
    "        new_row = {\n",
    "            'seed': random_state,\n",
    "            'n_outer': i,\n",
    "            'scorer': 'weighted_f1_score',\n",
    "            'cv_training_score': round(randomized_search.best_score_, 2),\n",
    "            'test_score': round(weighted_f1_score(y_test, y_pred), 2),\n",
    "            'f1': round(f1_score(y_test, y_pred), 2),\n",
    "            'accuracy': round(accuracy_score(y_test, y_pred), 2),\n",
    "            'recall': round(recall_score(y_test, y_pred), 2),\n",
    "            'precision': round(precision_score(y_test, y_pred), 2),\n",
    "            'roc_auc': round(roc_auc_score(y_test, y_pred), 2),\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist()  # Convert confusion matrix to a list\n",
    "        }\n",
    "        df_model_kpis = pd.concat([df_model_kpis, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        new_row = {\n",
    "            'seed': random_state,\n",
    "            'n_outer': i,\n",
    "            **dict(zip(X_train.columns, importances))\n",
    "        }\n",
    "        df_ft_imp = pd.concat([df_ft_imp, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        new_row = {\n",
    "            'seed': random_state,\n",
    "            'n_outer': i,\n",
    "            **randomized_search.best_params_\n",
    "        }\n",
    "        df_hyper_params = pd.concat([df_hyper_params, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    # remove duplicate entries (same combination of seed and n_outer), keep the newest entry\n",
    "    df_model_kpis = df_model_kpis.drop_duplicates(subset=['seed', 'n_outer'], keep='last')\n",
    "    df_ft_imp = df_ft_imp.drop_duplicates(subset=['seed', 'n_outer'], keep='last')\n",
    "    df_hyper_params = df_hyper_params.drop_duplicates(subset=['seed', 'n_outer'], keep='last')\n",
    "\n",
    "    #save df_model_kpis, df_ft_imp and df_hyper_params\n",
    "    df_model_kpis.to_pickle(file_name_results)\n",
    "    df_ft_imp.to_pickle(file_name_importances)\n",
    "    df_hyper_params.to_pickle(file_name_hyper_params)\n",
    "\n",
    "def show_results_from_nested_cv_rf():\n",
    "    filename_training_results = \"models/df_training_results.pkl\"\n",
    "    filename_ft_importances = \"models/df_ft_imp.pkl\"\n",
    "    filename_hyper_params = \"models/df_hyper_params.pkl\"\n",
    "    df_training_results = pd.read_pickle(filename_training_results)\n",
    "    df_ft_imp = pd.read_pickle(filename_ft_importances)\n",
    "    df_hyper_params = pd.read_pickle(filename_hyper_params)\n",
    "\n",
    "    # # only use last 50 rows\n",
    "    # df_training_results = df_training_results.tail(50)\n",
    "    # df_ft_imp = df_ft_imp.tail(50)\n",
    "    # df_hyper_params = df_hyper_params.tail(50)\n",
    "\n",
    "\n",
    "\n",
    "    # plotte Verteilung der Accuracy, des F1 Scores und der Recall Scores √ºber verschiedene seeds\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    accuracys = df_training_results['accuracy']\n",
    "    f1_scores = df_training_results['f1']\n",
    "    recalls = df_training_results['recall']\n",
    "    roc_auc_scores = df_training_results['roc_auc']\n",
    "    ax[0,0].hist(accuracys, color='skyblue', edgecolor='black')\n",
    "    ax[0,0].set_title('Accuracy')\n",
    "    ax[0,0].set_ylabel('Frequency')\n",
    "    ax[0,1].hist(f1_scores,  color='skyblue', edgecolor='black')\n",
    "    ax[0,1].set_title('F1 Score')\n",
    "    ax[0,1].set_ylabel('Frequency')\n",
    "    ax[1,0].hist(recalls,  color='skyblue', edgecolor='black')\n",
    "    ax[1,0].set_title('Recall')\n",
    "    ax[1,0].set_ylabel('Frequency')\n",
    "    ax[1,1].hist(roc_auc_scores, color='skyblue', edgecolor='black')\n",
    "    ax[1,1].set_title('ROC AUC')\n",
    "    ax[1,1].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # plotte pro hyperparameter die Verteilung der Feature Importances\n",
    "    fig, ax = plt.subplots(4, 4, figsize=(16, 16))\n",
    "\n",
    "    ft_imps = {}\n",
    "    # sum up the values of each column\n",
    "    for i, col in enumerate(df_ft_imp.columns[3:]):\n",
    "        sum_ft = df_ft_imp[col].sum()\n",
    "        ft_imps[col] = sum_ft\n",
    "\n",
    "    # sort the dictionary by value\n",
    "    ft_imps = dict(sorted(ft_imps.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # print order and list of features\n",
    "    print(\"Mean Feature Importances\")\n",
    "    print(ft_imps.keys())\n",
    "    for i, key in enumerate(ft_imps.keys()):\n",
    "        print(i, key,round(100*ft_imps[key]/len(df_ft_imp),2))\n",
    "\n",
    "    # top 15 features:\n",
    "    top_ft = list(ft_imps.keys())[:15]\n",
    "    for i, column in enumerate(top_ft):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        ax[row, col].hist(df_ft_imp[column], bins=10, color='skyblue', edgecolor='black')\n",
    "        ax[row, col].set_title(column)\n",
    "        # set_xlabel('Feature Importance')\n",
    "        ax[row, col].set_xlim(0, 0.2)\n",
    "        ax[row, col].set_xlabel('Feature Importance')\n",
    "        ax[row, col].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # plotte Verteilung der Hyperparameter\n",
    "    fig, ax = plt.subplots(3, 3, figsize=(16, 16))\n",
    "\n",
    "    # print(len(df_hyper_params.columns))\n",
    "\n",
    "    for i, column in enumerate(df_hyper_params.columns[2:]):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        if column == \"bootstrap\":\n",
    "            #replace True with 1 and False with 0\n",
    "            df_hyper_params[column] = df_hyper_params[column].replace({True: 1, False: 0})\n",
    "\n",
    "        if column not in [\"bootstrap\",\"criterion\",\"max_features\",\"class_weight\"]:\n",
    "            # one bin per integer\n",
    "            bins = np.arange(\n",
    "                min(df_hyper_params[column]),\n",
    "                max(df_hyper_params[column]) + 1,\n",
    "                1\n",
    "            ) - 0.5\n",
    "            ax[row, col].hist(df_hyper_params[column], color='skyblue', edgecolor='black', bins = bins)\n",
    "            print(f\"Mean of {column}: {round(df_hyper_params[column].mean(),1)}\")\n",
    "            print(f\"Standard Deviation: {round(df_hyper_params[column].std(),1)}\")\n",
    "        else:\n",
    "            ax[row, col].hist(df_hyper_params[column], color='skyblue', edgecolor='black')\n",
    "\n",
    "\n",
    "        ax[row, col].set_title(column)\n",
    "        ax[row, col].set_ylabel('Frequency')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(3, 3, figsize=(16, 16))\n",
    "    for i, column in enumerate(df_hyper_params.columns[2:]):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        if column == \"bootstrap\":\n",
    "            #replace True with 1 and False with 0\n",
    "            df_hyper_params[column] = df_hyper_params[column].replace({True: 1, False: 0})\n",
    "        col_values = df_hyper_params[column].values\n",
    "        col_results = df_training_results['test_score'].values\n",
    "        ax[row, col].scatter(col_values, col_results, color='skyblue',label=\"Custom\")\n",
    "        col_results = df_training_results['f1'].values\n",
    "        ax[row, col].scatter(col_values, col_results, color='red', label=\"F1\")\n",
    "        col_results = df_training_results['accuracy'].values\n",
    "        ax[row, col].scatter(col_values, col_results, color='green', label = \"Acc.\")\n",
    "        ax[row, col].set_title(column)\n",
    "        ax[row, col].set_ylabel('Score')\n",
    "        if row == 0 and col == 0:\n",
    "            ax[row, col].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # plotte Verteilung der Hyperparameter\n",
    "def randomized_search_cv_xgboost(data, features, n_folds, n_iter, random_state=42):\n",
    "\n",
    "    if n_folds < 3:\n",
    "        raise ValueError(\"n_folds must be at least 3 (need training, validation and testset)\")\n",
    "\n",
    "    # Definiere Suchbereich\n",
    "    param_dist = {\n",
    "        'objective': ['binary:logistic'],  # Loss Function\n",
    "        'eval_metric': ['auc'],#,'aucpr'],  # Evaluation metric #'auc'\n",
    "        'scale_pos_weight': [20, 25, 30],  # [30, 50, 100, 150],            # Weight for positive class\n",
    "        'max_depth': randint(3, 20),  # Maximale Tiefe der einzelnen B√§ume\n",
    "        'n_estimators': np.arange(30, 200, 50),  # Anzahl B√§ume /xGB Rounds\n",
    "        'eta': [0.01,0.05,0.08,0.1,0.15,0.2,0.3,0.4],  # Learning rate w√§hrend Optimierung\n",
    "        # 'gamma': 0.1,                                     # Minimale Loss-verringerung, um einen split in einem Baum zu machen\n",
    "        'subsample': np.linspace(0.4, 0.8, 6),  # Subsampling for regularization\n",
    "        'colsample_bytree': np.linspace(0.15, 0.6, 6),  # Feature selection\n",
    "        'lambda': [0, 0.001, 0.01, 0.1, 1],  # L2 Regularization\n",
    "        'early_stopping_rounds': [10]  # Early stopping if loss function does not improve for more than 5 rounds\n",
    "    }\n",
    "    # Split Data\n",
    "    X = data[[\"MatchId\"] + features]\n",
    "    y = data[\"epv_success\"]\n",
    "\n",
    "    # splitte Daten train: 64%, validation: 16%, test: 20%)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data_train_val_test(X,y,X.columns,random_state=random_state, n_splits=5, remove_matchid=False)\n",
    "\n",
    "    # erstelle folds\n",
    "    fold = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "\n",
    "\n",
    "    # Dataframe f√ºr ergebnisse\n",
    "    columns = list(param_dist.keys()) + [\"custom_score_\" + str(k+1) for k in range(n_folds)] + [\"avg_logloss\", \"avg_accuracy\", \"avg_recall\", \"avg_precision\", \"avg_roc_auc\", \"avg_custom_score\"]\n",
    "    rs_results = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Ziehe Parameter-Kombinationen\n",
    "    param_samples = ParameterSampler(param_dist, n_iter=n_iter, random_state=random_state)\n",
    "    print(\"Training XGBoost with randomized search and cross-validation\")\n",
    "    print(\"Number of parameter combinations: \", n_iter)\n",
    "    print(\"Number of folds: \", n_folds)\n",
    "    print(\"Total number of models: \", n_iter * n_folds)\n",
    "    print(\"Number of features: \", len(features))\n",
    "\n",
    "\n",
    "    # Pro Parameter-Kombination: trainiere Modell √ºber alle folds (1 fold als validierung, 1 fold als test), nutze jeweils validation set f√ºr early stopping, test f√ºr evaluation\n",
    "    for i, params in enumerate(param_samples):\n",
    "        row = {**params}\n",
    "        avg_logloss = 0\n",
    "        avg_accuracy = 0\n",
    "        avg_recall = 0\n",
    "        avg_precision = 0\n",
    "        avg_roc_auc = 0\n",
    "        avg_custom_score = 0\n",
    "        for fold_idx, (train_val_idx, test_idx) in enumerate(fold.split(X_train, y_train, groups=X_train['MatchId'])):\n",
    "            # time_stamp = datetime.datetime.now()\n",
    "\n",
    "            X_tr_vl, X_tst = X.iloc[train_val_idx], X.iloc[test_idx]\n",
    "            y_tr_vl, y_tst = y.iloc[train_val_idx], y.iloc[test_idx]\n",
    "            X_tst = X_tst.drop(columns=['MatchId'])\n",
    "            # teile train_val in train und validation, nur ein mal\n",
    "            for j, (train_idx, val_idx) in enumerate(fold.split(X_tr_vl, y_tr_vl, groups=X_tr_vl['MatchId'])):\n",
    "                X_tr, X_vl = X_tr_vl.iloc[train_idx], X_tr_vl.iloc[val_idx]\n",
    "                y_tr, y_vl = y_tr_vl.iloc[train_idx], y_tr_vl.iloc[val_idx]\n",
    "                X_tr = X_tr.drop(columns=['MatchId'])\n",
    "                X_vl = X_vl.drop(columns=['MatchId'])\n",
    "                break\n",
    "            # time_stamp_delta = datetime.datetime.now() - time_stamp\n",
    "            # time_stamp = datetime.datetime.now()\n",
    "            # print(f\"Time for splitting: {time_stamp_delta.total_seconds() * 1000:.2f} ms\")\n",
    "            #\n",
    "            # print(f\"Parameter {i+1}/{n_iter}, Fold {fold_idx+1}/{n_folds}\")\n",
    "            # print(\"Train: \", round(100* len(y_tr)/len(y_train),1), \"%, Validation: \", round(100* len(y_vl)/len(y_train),1), \"%, Test: \", round(100* len(y_tst)/len(y_train),1), \"%\")\n",
    "\n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_vl, y_vl)],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Compute log loss on test set\n",
    "            y_pred_prob = model.predict_proba(X_tst)[:, 1]\n",
    "            test_log_loss = log_loss(y_tst, y_pred_prob)\n",
    "            auc = roc_auc_score(y_tst, y_pred_prob)\n",
    "\n",
    "            # compute recall, precision, f1, accuracy\n",
    "            y_pred = model.predict(X_tst)\n",
    "            accuracy = accuracy_score(y_tst, y_pred)\n",
    "            recall = recall_score(y_tst, y_pred)\n",
    "            precision = precision_score(y_tst, y_pred)\n",
    "            custom_score = custom_scorer(y_tst, y_pred)\n",
    "\n",
    "            row[\"custom_score_\" + str(fold_idx+1)] = custom_score\n",
    "            avg_recall += recall\n",
    "            avg_precision += precision\n",
    "            avg_accuracy += accuracy\n",
    "            avg_logloss += test_log_loss\n",
    "            avg_roc_auc += auc\n",
    "            avg_custom_score += custom_score\n",
    "\n",
    "            # time_stamp_delta = datetime.datetime.now() - time_stamp\n",
    "            # print(f\"Time for training: {time_stamp_delta.total_seconds() * 1000:.2f} ms\")\n",
    "\n",
    "        avg_recall /= n_folds\n",
    "        avg_precision /= n_folds\n",
    "        avg_accuracy /= n_folds\n",
    "        avg_logloss /= n_folds\n",
    "        avg_roc_auc /= n_folds\n",
    "        avg_custom_score /= n_folds\n",
    "        row[\"avg_logloss\"] = avg_logloss\n",
    "        row[\"avg_accuracy\"] = avg_accuracy\n",
    "        row[\"avg_recall\"] = avg_recall\n",
    "        row[\"avg_precision\"] = avg_precision\n",
    "        row[\"avg_roc_auc\"] = avg_roc_auc\n",
    "        row[\"avg_custom_score\"] = avg_custom_score\n",
    "        rs_results = pd.concat([rs_results, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "    X_train = X_train.drop(columns=['MatchId'])\n",
    "    X_test = X_test.drop(columns=['MatchId'])\n",
    "    X_val = X_val.drop(columns=['MatchId'])\n",
    "    best_row = rs_results.loc[rs_results[\"avg_custom_score\"].idxmax()]\n",
    "    best_params = {param: best_row[param] for param in param_dist.keys()}\n",
    "    # trainiere bestes Modell auf allen Trainingsdaten\n",
    "    model = xgb.XGBClassifier(**best_params, random_state=random_state)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "    # save best model\n",
    "    if len(features) > 30:\n",
    "        name = 'models/epv_setpiece_allFeatures_xgboost.pkl'\n",
    "    else:\n",
    "        name = f'models/epv_setpiece_best{len(features)}Features_xgboost.pkl'\n",
    "    joblib.dump(model, name)\n",
    "    print(\"Best Model saved\")\n",
    "    print(\"Best Avg Custom Score: \", best_row[\"avg_custom_score\"])\n",
    "    print(\"Custom Scores per fold\", [round(best_row[\"custom_score_\" + str(k+1)],2) for k in range(n_folds)])\n",
    "    print(\"Best Parameters: \", best_params)\n",
    "\n",
    "    # evaluate best model on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf = confusion_matrix(y_test, y_pred)\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "    log_loss_score = log_loss(y_test, y_pred_prob)\n",
    "\n",
    "    print(\"Test Set Results:\")\n",
    "    print(\"Custom Score: \", custom_scorer(y_test, y_pred))\n",
    "\n",
    "    print(\"Prior Probability (True vs. Model):  \", round(sum(y_test) / len(y_test),2), round(sum(y_pred) / len(y_pred),2))\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Log Loss: \", log_loss_score)\n",
    "    print(\"Area under curve: \", roc_auc_score(y_test, y_pred_prob))\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"F1: \", f1)\n",
    "    print(\"Confusion Matrix: \", conf)\n",
    "    print(\"Classification Report: \", classification_report(y_test, y_pred))\n",
    "\n",
    "    # plotte hyperparameter in Bezug auf custom score\n",
    "    fig, ax = plt.subplots(3, 3, figsize=(16, 16))\n",
    "    for i, column in enumerate(param_dist.keys()):\n",
    "        if i < 9:\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            col_values = rs_results[column].values\n",
    "            col_results = rs_results['avg_custom_score'].values\n",
    "            ax[row, col].scatter(col_values, col_results, color='skyblue')\n",
    "            ax[row, col].set_title(column)\n",
    "            ax[row, col].set_ylabel('Custom Score')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # plotte ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.plot(fpr, tpr, label=\"roc Kurve\")\n",
    "    # bisective\n",
    "    ax.plot([0.0, 0.5, 1.0], [0.0, 0.5, 1.0], label=\"Diagonale\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # speichere Ergebnisse √ºber alle Parameter-Kombinationen\n",
    "    rs_results.to_pickle(\"models/xgboost_hyperparametersearchresults.pkl\")\n",
    "\n",
    "    # ##### BEST PARAMS #####\n",
    "    # best = {'objective': 'binary:logistic', 'eval_metric': 'auc', 'scale_pos_weight': 20, 'max_depth': 4, 'n_estimators': 200,\n",
    "    #  'eta': np.float64(0.05), 'subsample': np.float64(0.7), 'colsample_bytree': np.float64(0.5),\n",
    "    #  'lambda': np.float64(0.1), 'early_stopping_rounds': 10}\n",
    "\n",
    "    # {'objective': 'binary:logistic', 'eval_metric': 'auc', 'scale_pos_weight': 20, 'max_depth': 3, 'n_estimators': 50, 'eta': np.float64(0.05), 'subsample': np.float64(0.64), 'colsample_bytree': np.float64(0.51), 'lambda': np.float64(10.0), 'early_stopping_rounds': 10}\n",
    "\n",
    "    # Feature Importance\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    features = X_train.columns\n",
    "    features = features[indices]\n",
    "    # Print the feature importances of the forest\n",
    "    for ft in range(len(features)):\n",
    "        print(f\"{features[ft]}: {round(importances[indices[ft]], 3)}\")\n",
    "\n",
    "    top15fts = features[:15]\n",
    "    print(top15fts)\n",
    "\n",
    "    # SHAP Values\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap.summary_plot(shap_values, X_test)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# def randomized_search_cv_random_forrest(data, features, scorer, n_folds, n_iter, random_state):\n",
    "#     # Define the parameter grid\n",
    "#     param_dist = {\n",
    "#         \"max_features\": [\"sqrt\"],  # [\"sqrt\", \"log2\"],\n",
    "#         \"max_depth\": randint(6, 15),  # range(5, 20, 2)\n",
    "#         \"min_samples_split\": randint(2, 4),  # [2, 3, 5, 7]  # [2,4,6,8,10]\n",
    "#         \"min_samples_leaf\": randint(2, 4),\n",
    "#         \"class_weight\": [\"balanced\", \"balanced_subsample\"],\n",
    "#         # [\"balanced_subsample\", \"balanced\",{0: 1, 1:50}], #[\"balanced\", \"balanced_subsample\", {0: 1, 1:50}],# [\"balanced_subsample\", \"balanced\", {0: 1, 1: 30}, {0: 1, 1: 50}]# [None, \"balanced_subsample\", \"balanced\"]\n",
    "#         \"n_estimators\": randint(10, 50),  # range (20, 80, 10) # number of trees in the random forest\n",
    "#         \"bootstrap\": [True],  # [True, False],  # method used to sample data points\n",
    "#         \"criterion\": [\"gini\"]  # [\"gini\", \"entropy\"] # function to measure the quality\n",
    "#     }\n",
    "\n",
    "#     # Split Data\n",
    "#     X = data[[\"MatchId\"] + features]\n",
    "#     y = data[\"epv_success\"]\n",
    "#     X_train, X_val, X_test, y_train, y_val, y_test = split_data_train_val_test(X, y, X.columns,\n",
    "#                                                                                random_state=random_state)\n",
    "#     # F√ºr RF nur Test und Training Set (Training benutzt Cross Validation)\n",
    "#     X_train = pd.concat([X_train, X_val], axis=0)\n",
    "#     y_train = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "#     # groups for stratified group k fold\n",
    "#     groups = X_train['MatchId']\n",
    "\n",
    "#     # remove MatchId\n",
    "#     X_train = X_train.drop(columns=['MatchId'])\n",
    "#     X_test = X_test.drop(columns=['MatchId'])\n",
    "\n",
    "#     # Stratified Folds mit getrennten MatchIds f√ºr Cross-Validation\n",
    "#     fold = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "#     rf_clf = RandomForestClassifier(random_state=random_state)\n",
    "#     # Perform Randomized Search\n",
    "#     clf_search = RandomizedSearchCV(\n",
    "#         estimator=rf_clf,\n",
    "#         param_distributions=param_dist,\n",
    "#         n_iter=n_iter,\n",
    "#         scoring=scorer,#make_scorer(roc_auc_score, needs_proba=True),\n",
    "#         cv=fold,\n",
    "#         n_jobs=-1,\n",
    "#         verbose=1,\n",
    "#         random_state=random_state\n",
    "#     )\n",
    "\n",
    "#     rf_clf = clf_search.fit(X_train, y_train, groups=groups).best_estimator_\n",
    "\n",
    "#     # save best model\n",
    "#     results_df = pd.DataFrame(clf_search.cv_results_)\n",
    "#     print(\"Best Parameters: \", clf_search.best_params_)\n",
    "#     print(\"Best CV Score: \", clf_search.best_score_)\n",
    "#     if len(features) > 30:\n",
    "#         name = 'models/epv_setpiece_allFeatures_randomForrest.pkl'\n",
    "#     else:\n",
    "#         name = f'models/epv_setpiece_best{len(features)}Features_randomForrest.pkl'\n",
    "#     joblib.dump(rf_clf, name)\n",
    "#     print(\"Best Model saved\")\n",
    "\n",
    "#     y_pred = rf_clf.predict(X_test)\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     recall = recall_score(y_test, y_pred)\n",
    "#     precision = precision_score(y_test, y_pred)\n",
    "#     f1 = f1_score(y_test, y_pred)\n",
    "#     conf = confusion_matrix(y_test, y_pred)\n",
    "#     y_pred_prob = rf_clf.predict_proba(X_test)[:, 1]\n",
    "#     log_loss_score = log_loss(y_test, y_pred_prob)\n",
    "\n",
    "#     print(\"Prior Probability (True vs. Model):  \", round(sum(y_test) / len(y_test),2), round(sum(y_pred) / len(y_pred),2))\n",
    "#     print(\"Accuracy: \", accuracy)\n",
    "#     print(\"Log Loss: \", log_loss_score)\n",
    "#     print(\"Area under curve: \", roc_auc_score(y_test, y_pred_prob))\n",
    "#     print(\"Recall: \", recall)\n",
    "#     print(\"Precision: \", precision)\n",
    "#     print(\"F1: \", f1)\n",
    "#     print(\"Confusion Matrix: \", conf)\n",
    "#     print(\"Classification Report: \", classification_report(y_test, y_pred))\n",
    "\n",
    "#     # ROC plot\n",
    "#     fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "#     fig, ax = plt.subplots(figsize=(5, 5))\n",
    "#     ax.plot(fpr, tpr, label=\"roc Kurve\")\n",
    "#     # plot bisective\n",
    "#     ax.plot([0.0, 0.5, 1.0], [0.0, 0.5, 1.0], label=\"Diagonale\")\n",
    "\n",
    "#     ax.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Plot Recall and Accuracy over different thresholds\n",
    "#     recalls = [recall_score(y_test, y_pred_prob > threshold) for threshold in thresholds]\n",
    "#     accuracies = [accuracy_score(y_test, y_pred_prob > threshold) for threshold in thresholds]\n",
    "#     fig, ax = plt.subplots(figsize=(5, 5))\n",
    "#     ax.plot(thresholds, recalls, label=\"Recall\")\n",
    "#     ax.plot(thresholds, accuracies, label=\"Accuracy\")\n",
    "#     ax.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     # # Feature Importances\n",
    "#     importances = rf_clf.feature_importances_\n",
    "#     indices = np.argsort(importances)[::-1]\n",
    "#     features = X_train.columns\n",
    "#     features = features[indices]\n",
    "#     # Print the feature importances of the forest\n",
    "#     for ft in range(len(features)):\n",
    "#         print(f\"{features[ft]}: {round(importances[indices[ft]], 3)}\")\n",
    "\n",
    "#     top15fts = features[:15]\n",
    "#     print(top15fts)\n",
    "\n",
    "#     # SHAP Values\n",
    "#     explainer = shap.TreeExplainer(rf_clf)\n",
    "#     # # Compute SHAP values for the dataset\n",
    "#     shap_values = explainer.shap_values(X_test)\n",
    "#     shap_values = shap_values[:,:,1]\n",
    "\n",
    "\n",
    "#     # Plot the SHAP values\n",
    "#     shap.summary_plot(shap_values, X_test)\n",
    "#     plt.show()\n",
    "\n",
    "# class MLP_small(nn.Module):\n",
    "#     # Input -> 32 -> 16 -> 1 -> Sigmoid\n",
    "#     def __init__(self, input_size, dropout_rate=0.0):\n",
    "#         super(MLP_small, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, 32)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout1 = nn.Dropout(dropout_rate)\n",
    "#         self.fc2 = nn.Linear(32, 16)\n",
    "#         self.dropout2 = nn.Dropout(dropout_rate)\n",
    "#         self.fc3 = nn.Linear(16, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()  # Binary classification\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.dropout1(x)\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.sigmoid(self.fc3(x))\n",
    "#         return x\n",
    "\n",
    "# class MLP_medium(nn.Module):\n",
    "#     # Input -> 64 -> 32 -> 1 -> Sigmoid\n",
    "#     def __init__(self, input_size, dropout_rate=0.0):\n",
    "#         super(MLP_medium, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, 64)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout1 = nn.Dropout(dropout_rate)\n",
    "#         self.fc2 = nn.Linear(64, 32)\n",
    "#         self.dropout2 = nn.Dropout(dropout_rate)\n",
    "#         self.fc3 = nn.Linear(32, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()  # Binary classification\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.dropout1(x)\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.sigmoid(self.fc3(x))\n",
    "#         return x\n",
    "\n",
    "# class MLP_large(nn.Module):\n",
    "#     # Input -> 64 -> 32 -> 16 -> 1 -> Sigmoid\n",
    "#     def __init__(self, input_size,  dropout_rate=0.0):\n",
    "#         super(MLP_large, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, 64)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout1 = nn.Dropout(dropout_rate)\n",
    "#         self.fc2 = nn.Linear(64, 32)\n",
    "#         self.dropout2 = nn.Dropout(dropout_rate)\n",
    "#         self.fc3 = nn.Linear(32, 16)\n",
    "#         self.dropout3 = nn.Dropout(dropout_rate)\n",
    "#         self.fc4 = nn.Linear(16, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()  # Binary classification\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.dropout1(x)\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.relu(self.fc3(x))\n",
    "#         x = self.dropout3(x)\n",
    "#         x = self.sigmoid(self.fc4(x))\n",
    "#         return x\n",
    "\n",
    "# def randomized_search_cv_nn(data, features, n_folds, n_iter, random_state=42):\n",
    "#     if n_folds < 3:\n",
    "#         raise ValueError(\"n_folds must be at least 3 (need training, validation and testset)\")\n",
    "\n",
    "#     # Definiere Suchbereich\n",
    "#     param_dist = {\n",
    "#         'architecture': ['small'],#['small','medium'],#['small', 'medium', 'large'],\n",
    "#         'batch_size': [32,64],#[16, 32, 64, 128],\n",
    "#         'learning_rate': [0.0005,0.001],#[0.00005, 0.0001, 0.0005, 0.001, 0.005],\n",
    "#         'weight_decay': [0, 0.001, 0.01],\n",
    "#         'dropout': [0.1,0.2,0.3],#[0, 0.05, 0.1, 0.2, 0.3, 0.4],\n",
    "#         'pos_weight': [23],#[20, 22, 25, 27, 30, 32],\n",
    "#         'epochs': [30,60,90,120],#[30, 45, 60, 75, 90, 105, 120],\n",
    "#         'early_stopping': [True],#[True, False],\n",
    "#         'criterion': ['BCELossLogits']#['BCELoss', 'BCELossLogits']\n",
    "#     }\n",
    "#     # Split Data\n",
    "#     X = data[[\"MatchId\"] + features]\n",
    "#     y = data[\"epv_success\"]\n",
    "\n",
    "#     # splitte Daten train: 64% (cross validation), validation: 16% (early stopping), test: 20%)\n",
    "#     X_train, X_val, X_test, y_train, y_val, y_test = split_data_train_val_test(X,y,X.columns,random_state=random_state, n_splits=5, remove_matchid=False)\n",
    "\n",
    "#     # erstelle folds\n",
    "#     fold = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "#     # Dataframe f√ºr ergebnisse\n",
    "#     columns = list(param_dist.keys()) + [\"custom_score_\" + str(k+1) for k in range(n_folds)] + [\"avg_logloss\", \"avg_accuracy\", \"avg_recall\", \"avg_precision\", \"avg_roc_auc\", \"avg_custom_score\"]\n",
    "#     rs_results = pd.DataFrame(columns=columns)\n",
    "\n",
    "#     # Ziehe Parameter-Kombinationen\n",
    "#     param_samples = ParameterSampler(param_dist, n_iter=n_iter, random_state=random_state)\n",
    "#     print(\"Training MLP with randomized search and cross-validation\")\n",
    "#     print(\"Number of parameter combinations: \", n_iter)\n",
    "#     print(\"Number of folds: \", n_folds)\n",
    "#     print(\"Total number of models: \", n_iter * n_folds)\n",
    "#     print(\"Number of features: \", len(features))\n",
    "\n",
    "#     start_time_stamp_overall = datetime.datetime.now()\n",
    "#     # Pro Parameter-Kombination: trainiere Modell √ºber alle folds (1 fold als validierung, 1 fold als test), nutze jeweils validation set f√ºr early stopping, test f√ºr evaluation\n",
    "#     for i, params in enumerate(param_samples):\n",
    "#         if i > 0:\n",
    "#             end_time_stamp = datetime.datetime.now()\n",
    "#             print(f\"Parameters {i + 1}/{n_iter}: Runtime {(end_time_stamp - start_timestamp).seconds} s\")\n",
    "#         start_timestamp = datetime.datetime.now()\n",
    "#         row = {**params}\n",
    "#         avg_logloss = 0\n",
    "#         avg_accuracy = 0\n",
    "#         avg_recall = 0\n",
    "#         avg_precision = 0\n",
    "#         avg_roc_auc = 0\n",
    "#         avg_custom_score = 0\n",
    "\n",
    "#         for fold_idx, (train_val_idx, test_idx) in enumerate(fold.split(X_train, y_train, groups=X_train['MatchId'])):\n",
    "\n",
    "#             X_tr_vl, X_tst = X.iloc[train_val_idx], X.iloc[test_idx]\n",
    "#             y_tr_vl, y_tst = y.iloc[train_val_idx], y.iloc[test_idx]\n",
    "#             X_tst = X_tst.drop(columns=['MatchId'])\n",
    "\n",
    "#             # teile train_val in train und validation, nur ein mal\n",
    "#             for j, (train_idx, val_idx) in enumerate(fold.split(X_tr_vl, y_tr_vl, groups=X_tr_vl['MatchId'])):\n",
    "#                 X_tr, X_vl = X_tr_vl.iloc[train_idx], X_tr_vl.iloc[val_idx]\n",
    "#                 y_tr, y_vl = y_tr_vl.iloc[train_idx], y_tr_vl.iloc[val_idx]\n",
    "#                 X_tr = X_tr.drop(columns=['MatchId'])\n",
    "#                 X_vl = X_vl.drop(columns=['MatchId'])\n",
    "#                 break\n",
    "\n",
    "#             # Konvertiere Dataframes zu PyTorch tensors, normalisiere features\n",
    "#             scaler = StandardScaler()\n",
    "#             X_tr_tensor = torch.tensor(scaler.fit_transform(X_tr), dtype=torch.float32)\n",
    "#             y_tr_tensor = torch.tensor(y_tr.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "#             X_vl_tensor = torch.tensor(scaler.fit_transform(X_vl), dtype=torch.float32)\n",
    "#             y_vl_tensor = torch.tensor(y_vl.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "#             X_tst_tensor = torch.tensor(scaler.fit_transform(X_tst), dtype=torch.float32)\n",
    "#             y_tst_tensor = torch.tensor(y_tst.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "#             # Create DataLoader for batching\n",
    "#             train_data = TensorDataset(X_tr_tensor, y_tr_tensor)\n",
    "#             train_loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)\n",
    "\n",
    "#             # Modell architektur\n",
    "#             if params['architecture'] == 'small':\n",
    "#                 model = MLP_small(input_size=len(features), dropout_rate=params['dropout'])\n",
    "#             elif params['architecture'] == 'medium':\n",
    "#                 model = MLP_medium(input_size=len(features), dropout_rate=params['dropout'])\n",
    "#             else:\n",
    "#                 model = MLP_large(input_size=len(features), dropout_rate=params['dropout'])\n",
    "\n",
    "#             # Loss Function\n",
    "#             if params['criterion'] == \"BCELoss\":\n",
    "\n",
    "#                 def custom_bce_loss(y_pred_prob, y_true):\n",
    "#                     w1 = params['pos_weight']\n",
    "#                     w0 = 1 / (1 + w1)\n",
    "#                     w1 = w1 / (1 + w1)\n",
    "#                     crit = nn.BCELoss(reduction='none')\n",
    "#                     ls = crit(y_pred_prob, y_true)\n",
    "#                     w_ls = ls * (y_true * w1 + (1 - y_true) * w0)\n",
    "#                     final_ls = w_ls.mean()\n",
    "#                     return final_ls\n",
    "\n",
    "#                 criterion = custom_bce_loss\n",
    "\n",
    "#                 print(\"custom loss\")\n",
    "\n",
    "#             else:\n",
    "#                 criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(params['pos_weight']))\n",
    "\n",
    "#             # Optimizer\n",
    "#             optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "\n",
    "#             num_epochs = params['epochs']\n",
    "\n",
    "#             validation_loss = []\n",
    "#             # Trainiere Modell\n",
    "#             for epoch in range(num_epochs):\n",
    "#                 for inputs, labels in train_loader:\n",
    "#                     optimizer.zero_grad()\n",
    "#                     outputs = model(inputs)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "#                     loss.backward()\n",
    "#                     optimizer.step()\n",
    "\n",
    "#                 with torch.no_grad():\n",
    "#                     y_vl_pred = model(X_vl_tensor)\n",
    "#                     y_tr_pred = model(X_tr_tensor)\n",
    "#                 train_auc = roc_auc_score(y_tr_tensor, y_tr_pred)\n",
    "#                 train_loss = criterion(y_tr_pred, y_tr_tensor)\n",
    "#                 val_auc = roc_auc_score(y_vl_tensor, y_vl_pred)\n",
    "#                 val_loss = criterion(y_vl_pred, y_vl_tensor)\n",
    "#                 validation_loss.append(val_loss)\n",
    "#                 print(\n",
    "#                     f\"Epoch [{epoch + 1}/{num_epochs}], auc on training set: {train_auc:.2f}, Loss: {train_loss:.6f}, auc on validation set: {val_auc:.2f}, validation loss: {val_loss:.6f}\")\n",
    "#                 if params['early_stopping']:\n",
    "#                     if len(validation_loss) > 6 and val_loss >= max(validation_loss[-6:-1]) - 0.0001:\n",
    "#                         # print(\"Early Stopping done after \", epoch, \" epochs\")\n",
    "#                         break\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 y_tst_pred_prob = model(X_tst_tensor)\n",
    "#                 y_tst_pred = (y_tst_pred_prob > 0.5).float()  # Convert probabilities to 0/1\n",
    "\n",
    "#             test_log_loss = log_loss(y_tst_tensor, y_tst_pred_prob)\n",
    "#             auc = roc_auc_score(y_tst_tensor, y_tst_pred_prob)\n",
    "\n",
    "#             # compute recall, precision, f1, accuracy\n",
    "\n",
    "#             if sum(y_tst_pred) > 0:\n",
    "#                 recall = recall_score(y_tst_tensor, y_tst_pred)\n",
    "#                 precision = precision_score(y_tst_tensor, y_tst_pred)\n",
    "#                 custom_score = custom_scorer(y_tst_tensor, y_tst_pred)\n",
    "#             else:\n",
    "#                 recall = 0\n",
    "#                 precision = 0\n",
    "#                 custom_score = 0\n",
    "#             accuracy = accuracy_score(y_tst_tensor, y_tst_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             row[\"custom_score_\" + str(fold_idx+1)] = custom_score\n",
    "#             avg_recall += recall\n",
    "#             avg_precision += precision\n",
    "#             avg_accuracy += accuracy\n",
    "#             avg_logloss += test_log_loss\n",
    "#             avg_roc_auc += auc\n",
    "#             avg_custom_score += custom_score\n",
    "\n",
    "#             # time_stamp_delta = datetime.datetime.now() - time_stamp\n",
    "#             # print(f\"Time for training: {time_stamp_delta.total_seconds() * 1000:.2f} ms\")\n",
    "\n",
    "\n",
    "#         avg_recall /= n_folds\n",
    "#         avg_precision /= n_folds\n",
    "#         avg_accuracy /= n_folds\n",
    "#         avg_logloss /= n_folds\n",
    "#         avg_roc_auc /= n_folds\n",
    "#         avg_custom_score /= n_folds\n",
    "\n",
    "#         if len(rs_results) > 0:\n",
    "#             if avg_custom_score > max(rs_results[\"avg_custom_score\"]):\n",
    "#                 print(\"New best avg custom score: \", f\"{avg_custom_score:.4f}, avg auc: { avg_roc_auc:.4f}, avg logloss: {avg_logloss:.4f}, avg recall: {avg_recall:.4f}, avg precision: {avg_precision:.4f}, avg accuracy: {avg_accuracy:.4f}\")\n",
    "#                 print(\"Parameters: \", params)\n",
    "#         row[\"avg_logloss\"] = avg_logloss\n",
    "#         row[\"avg_accuracy\"] = avg_accuracy\n",
    "#         row[\"avg_recall\"] = avg_recall\n",
    "#         row[\"avg_precision\"] = avg_precision\n",
    "#         row[\"avg_roc_auc\"] = avg_roc_auc\n",
    "#         row[\"avg_custom_score\"] = avg_custom_score\n",
    "\n",
    "\n",
    "#         rs_results = pd.concat([rs_results, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "#     end_time_stamp_overall = datetime.datetime.now()\n",
    "#     print(f\"Zeit f√ºr Parameter Suche: {(end_time_stamp_overall-start_time_stamp_overall).seconds} s\")\n",
    "\n",
    "#     # Konvertiere Dataframes zu PyTorch tensors, normalisiere features\n",
    "#     X_train = X_train.drop(columns=['MatchId'])\n",
    "#     X_test = X_test.drop(columns=['MatchId'])\n",
    "#     X_val = X_val.drop(columns=['MatchId'])\n",
    "\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_tensor = torch.tensor(scaler.fit_transform(X_train), dtype=torch.float32)\n",
    "#     y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "#     X_val_tensor = torch.tensor(scaler.transform(X_val), dtype=torch.float32)\n",
    "#     y_val_tensor = torch.tensor(y_val.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "#     X_test_tensor = torch.tensor(scaler.transform(X_test), dtype=torch.float32)\n",
    "#     y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "#     # Create DataLoader for batching\n",
    "#     train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "#     train_loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)\n",
    "\n",
    "#     # Trainiere und speichere bestes Modell\n",
    "#     rs_results.to_pickle(\"models/nn_hyperparametersearchresults_top15.pkl\")\n",
    "#     best_row = rs_results.loc[rs_results[\"avg_custom_score\"].idxmax()]\n",
    "#     best_params = {param: best_row[param] for param in param_dist.keys()}\n",
    "\n",
    "#     # best_params = {'architecture': 'medium', 'batch_size': 64, 'learning_rate': np.float64(0.0001), 'weight_decay': np.float64(0.01), 'dropout': np.float64(0.2), 'pos_weight': 27, 'epochs': 120, 'early_stopping': True, 'criterion': 'BCELossLogits'}\n",
    "\n",
    "#     if best_params['architecture'] == 'small':\n",
    "#         model = MLP_small(input_size=len(features), dropout_rate=best_params['dropout'])\n",
    "#     elif best_params['architecture'] == 'medium':\n",
    "#         model = MLP_medium(input_size=len(features), dropout_rate=best_params['dropout'])\n",
    "#     else:\n",
    "#         model = MLP_large(input_size=len(features), dropout_rate=best_params['dropout'])\n",
    "\n",
    "#     # Loss Function\n",
    "#     if best_params['criterion'] == \"BCELoss\":\n",
    "\n",
    "#         def custom_bce_loss(y_pred_prob, y_true):\n",
    "#             w1 = params['pos_weight']\n",
    "#             w0 = 1 / (1 + w1)\n",
    "#             w1 = w1 / (1 + w1)\n",
    "#             crit = nn.BCELoss(reduction='none')\n",
    "#             ls = crit(y_pred_prob, y_true)\n",
    "#             w_ls = ls * (y_true * w1 + (1 - y_true) * w0)\n",
    "#             final_ls = w_ls.mean()\n",
    "#             return final_ls\n",
    "\n",
    "#         criterion = custom_bce_loss\n",
    "#         print(\"custom loss\")\n",
    "\n",
    "#     else:\n",
    "#         criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(params['pos_weight']))\n",
    "\n",
    "#     # criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(best_params['pos_weight']))\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'],\n",
    "#                                  weight_decay=best_params['weight_decay'])\n",
    "\n",
    "#     num_epochs = best_params['epochs']\n",
    "#     loss_history = []\n",
    "#     validation_auc = []\n",
    "#     validation_loss = []\n",
    "\n",
    "#     # Trainiere Modell\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for inputs, labels in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             y_val_pred = model(X_val_tensor)\n",
    "#             y_train_pred = model(X_train_tensor)\n",
    "#         val_auc = roc_auc_score(y_val_tensor, y_val_pred)\n",
    "#         val_loss = criterion(y_val_pred, y_val_tensor)\n",
    "#         train_loss = criterion(y_train_pred, y_train_tensor)\n",
    "\n",
    "#         validation_auc.append(val_auc)\n",
    "#         validation_loss.append(val_loss)\n",
    "\n",
    "#         if params['early_stopping']:\n",
    "#             if len(validation_loss) > 6 and val_loss <= min(validation_loss[-6:-1]):\n",
    "#                 print(\"Early Stopping done after \", epoch, \" epochs\")\n",
    "#                 break\n",
    "\n",
    "\n",
    "#         print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}, auc on val set: {val_auc:.4f}, loss on val set: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "#     # save best model\n",
    "#     if len(features) > 30:\n",
    "#         name = 'models/epv_setpiece_allFeatures_nn.pkl'\n",
    "#         name_scaler = 'models/epv_setpiece_allFeatures_nn_scaler.pkl'\n",
    "#     else:\n",
    "#         name = f'models/epv_setpiece_best{len(features)}Features_nn.pkl'\n",
    "#         name_scaler = f'models/epv_setpiece_best{len(features)}Features_nn_scaler.pkl'\n",
    "#     joblib.dump(model, name)\n",
    "#     joblib.dump(scaler, name_scaler)\n",
    "\n",
    "#     print(\"Best Model saved\")\n",
    "#     print(\"Best Avg Custom Score: \", best_row[\"avg_custom_score\"])\n",
    "#     print(\"Custom Scores per fold\", [round(best_row[\"custom_score_\" + str(k+1)],2) for k in range(n_folds)])\n",
    "#     print(\"Best Parameters: \", best_params)\n",
    "\n",
    "#     # evaluate best model on test set\n",
    "#     with torch.no_grad():\n",
    "#         y_test_pred_prob = model(X_test_tensor)\n",
    "#         y_test_pred = (y_test_pred_prob > 0.5).float()\n",
    "\n",
    "#     log_loss_score = log_loss(y_test_tensor, y_test_pred_prob)\n",
    "#     auc = roc_auc_score(y_test_tensor, y_test_pred_prob)\n",
    "#     accuracy = accuracy_score(y_test_tensor, y_test_pred)\n",
    "\n",
    "#     if sum(y_test_pred) > 0:\n",
    "#         recall = recall_score(y_test_tensor, y_test_pred)\n",
    "#         precision = precision_score(y_test_tensor, y_test_pred)\n",
    "#         f1 = f1_score(y_test_tensor, y_test_pred)\n",
    "#         custom_score = custom_scorer(y_test_tensor, y_test_pred)\n",
    "#     else:\n",
    "#         recall = 0\n",
    "#         precision = 0\n",
    "#         f1 = 0\n",
    "#         custom_score = 0\n",
    "#     conf = confusion_matrix(y_test_tensor, y_test_pred)\n",
    "\n",
    "\n",
    "\n",
    "#     print(\"Test Set Results:\")\n",
    "#     print(\"Custom Score: \", custom_score)\n",
    "\n",
    "#     # print(\"Prior Probability (True vs. Model):  \", round(sum(y_test_tensor) / len(y_test_tensor),2), round(sum(y_test_pred) / len(y_test_pred),2))\n",
    "#     print(f\"Accuracy: {(100*accuracy):.2f}%\")\n",
    "#     print(f\"Log Loss: {(log_loss_score):.4f}\")\n",
    "#     print(f\"Area under curve: {(auc):.4f}\")\n",
    "#     print(f\"Recall: {(100 * recall):.2f}%\")\n",
    "#     print(f\"Precision: {(100 * precision):.2f}%\")\n",
    "#     print(f\"F1: {(100 * f1):.2f}%\")\n",
    "#     print(\"Confusion Matrix: \", conf)\n",
    "#     print(\"Classification Report: \", classification_report(y_test_tensor, y_test_pred))\n",
    "\n",
    "#     # boxplot\n",
    "#     n_hyper = len(param_dist.keys())\n",
    "#     max_per_row = 4\n",
    "#     rows = math.ceil(n_hyper/ max_per_row)\n",
    "\n",
    "#     fig, ax = plt.subplots(rows, max_per_row)\n",
    "#     df = rs_results\n",
    "#     for i, param in enumerate(\n",
    "#             param_dist.keys()):\n",
    "#         vals = df[param].unique()\n",
    "#         groups = [df[df[param] == val]['avg_custom_score'] for val in vals]\n",
    "\n",
    "#         row = i // max_per_row\n",
    "#         col = i % max_per_row\n",
    "#         ax[row, col].boxplot(groups, labels=vals)\n",
    "#         ax[row, col].set_title(param)\n",
    "#         ax[row, col].set_ylabel(\"Custom Score\")\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     # plotte ROC curve\n",
    "#     fpr, tpr, thresholds = roc_curve(y_test_tensor, y_test_pred_prob)\n",
    "#     fig, ax = plt.subplots(figsize=(5, 5))\n",
    "#     ax.plot(fpr, tpr, label=\"roc Kurve\")\n",
    "#     # bisective\n",
    "#     ax.plot([0.0, 0.5, 1.0], [0.0, 0.5, 1.0], label=\"Diagonale\")\n",
    "#     ax.legend()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     def model_wrapper(x):\n",
    "#         x_tensor = torch.tensor(x, dtype=torch.float32)  # Convert NumPy array to tensor\n",
    "#         with torch.no_grad():\n",
    "#             return model(x_tensor).numpy()  # Convert output back to NumPy\n",
    "\n",
    "\n",
    "\n",
    "#     # Convert input tensors to NumPy (SHAP requires NumPy arrays)\n",
    "#     X_train_np = X_train_tensor.numpy()\n",
    "#     X_test_np = X_test_tensor.numpy()\n",
    "\n",
    "#     # Adjust column names\n",
    "#     feature_names = X_train.columns\n",
    "\n",
    "#     explainer = shap.Explainer(model_wrapper, X_train_np, feature_names=feature_names)\n",
    "#     # Compute SHAP values\n",
    "#     shap_values = explainer(X_test_np)\n",
    "\n",
    "#     # Plot feature importance\n",
    "#     shap.summary_plot(shap_values, X_test_np)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# XGBoost-compatible evaluation function\n",
    "def xgb_custom_eval(y_pred_proba, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    return \"custom_score\", custom_scorer(y_true, y_pred_proba)\n",
    "\n",
    "def shap_plot_per_feature(fts, shap_values,X_test):\n",
    "\n",
    "    fig, ax = plt.subplots(3, 5, figsize=(20, 25))\n",
    "\n",
    "    for i in range(len(fts)):\n",
    "        ft = fts\n",
    "        svs = shap_values[:,i]\n",
    "        testdata = X_test[fts[i]].to_numpy()\n",
    "\n",
    "\n",
    "        #remove outliers from testdata\n",
    "        q1 = np.percentile(testdata, 25)\n",
    "        q3 = np.percentile(testdata, 75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        svs = svs[(testdata > lower_bound) & (testdata < upper_bound)]\n",
    "        testdata = testdata[(testdata > lower_bound) & (testdata < upper_bound)]\n",
    "\n",
    "        # ausgleichsgerade\n",
    "        x_s = np.linspace(min(testdata), max(testdata), 100)\n",
    "        z = np.polyfit(testdata, svs, 3)\n",
    "        p = np.poly1d(z)\n",
    "        ax[i//5, i%5].plot(x_s, p(x_s), \"r--\")\n",
    "        ax[i // 5, i % 5].scatter(testdata, svs, alpha=0.7, s=0.8)\n",
    "        ax[i // 5, i % 5].set_title(fts[i])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def predict_with_nn_model(model_file, scaler_file, X):\n",
    "    '''\n",
    "    Macht mit top 15 feature Neural Network eine Prediction,\n",
    "    Liefert Predicted Probability und Predicted Label\n",
    "    '''\n",
    "    top_15_features = ['sp_position_distance_to_goal',\n",
    "                       'sp_first_ball_action_ball_z_max',\n",
    "                       'sp_heights_shot_zone_defense_mean',\n",
    "                       'sp_delivery_distance',\n",
    "                       'sp_initial_last_line_of_defense',\n",
    "                       'sp_position_vertical_zone',\n",
    "                       'sp_delivery_velocity',\n",
    "                       'sp_delivery_end_position_distance_to_goal',\n",
    "                       'sp_defensive_pressure_shot_zone_mean',\n",
    "                       'sp_space_control_in_box_offense',\n",
    "                       'sp_delivery_time',\n",
    "                       'sp_position_angle_to_goal',\n",
    "                       'sp_heights_shot_zone_offense_count',\n",
    "                       'sp_delivery_end_position_angle_to_goal',\n",
    "                       'sp_space_control_in_dfb_shot_zone_offense']\n",
    "\n",
    "    X = X[top_15_features]\n",
    "\n",
    "    model = joblib.load(model_file)\n",
    "    scaler = joblib.load(scaler_file)\n",
    "\n",
    "    X_tensor = torch.tensor(scaler.transform(X), dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred_prob = model(X_tensor)\n",
    "        y_pred = (y_pred_prob > 0.5).float()\n",
    "\n",
    "    return y_pred_prob, y_pred\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f899d8",
   "metadata": {},
   "source": [
    "## 1. Split Dataset ##\n",
    "\n",
    "- Reserve 20% of Data samples for the evaluation of the best model (Overall Test Set) \n",
    "- Use the remaining 80% for Hyperparameter tuning with Cross-Validation and Training of the best model (If applicable to the Model architeture, use 16% as validation set for early stopping)\n",
    "- Perform grouped splits w.r.t. Match ID (Samples of same match are allways in the same set)\n",
    "- Perform stratified splits (distribution of goal / no goal samples is similar in each set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8491bb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information about the entire Dataset:\n",
      "Number of samples:  16955\n",
      "Number of positive samples:  678\n",
      "Number negative samples:  16277\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Samples</th>\n",
       "      <th>% Positive (y=1)</th>\n",
       "      <th>Distinct MatchIds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>10931</td>\n",
       "      <td>4.38</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>2502</td>\n",
       "      <td>4.08</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>3522</td>\n",
       "      <td>2.75</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall</th>\n",
       "      <td>16955</td>\n",
       "      <td>4.00</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Total Samples  % Positive (y=1)  Distinct MatchIds\n",
       "Train               10931              4.38                390\n",
       "Validation           2502              4.08                 98\n",
       "Test                 3522              2.75                122\n",
       "Overall             16955              4.00                610"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weight of positive class:  21.8\n",
      "No correlations above 0.75\n"
     ]
    }
   ],
   "source": [
    "random_state = 42 # For reproducibility!\n",
    "\n",
    "# Load preprocessed set piece data from precomputed pickle file\n",
    "events_processed = pd.read_pickle('events_compressed_processed.pkl')\n",
    "\n",
    "# Filter corner kick events, as only models for corner kicks are trained\n",
    "events_processed = events_processed[events_processed['sp_type'] == 1]\n",
    "\n",
    "# print amount of rows, positive and negative samples\n",
    "print(\"Information about the entire Dataset:\")\n",
    "print(\"Number of samples: \", len(events_processed))\n",
    "print(\"Number of positive samples: \", len(events_processed[events_processed['epv_success'] == 1]))\n",
    "print(\"Number negative samples: \", len(events_processed[events_processed['epv_success'] == 0]))\n",
    "\n",
    "# # drop column sp_type, as it is already filtered\n",
    "# cols_kpis_final_no_corr.remove('sp_type')\n",
    "\n",
    "#############################\n",
    "#############################\n",
    "####### 1. Split Data #######\n",
    "#############################\n",
    "#############################\n",
    "\n",
    "def split_data_train_val_test(X, y, cols, random_state=42, n_splits = 5, remove_matchid = True):\n",
    "    '''\n",
    "    Splits dataset (X,y) into stratified sets, grouped by MatchID,\n",
    "    Returns training set (64%), validation set (16%), test set (20%),\n",
    "    If not specified otherwise, the MatchId Column is kept (e.g. for further splitting during cross_validation),\n",
    "    otherwise only the predictor columns (cols) are kept in X_train, X_val, X_test\n",
    "    '''\n",
    "    # Initialize StratifiedGroupKFold with 2 splits\n",
    "    # One for training + validation, and the other for testing\n",
    "    strat_group_kfold = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # This will give stratified splits while keeping `MatchId` groups together\n",
    "    # We only want to use the first split (train/val, test split)\n",
    "    for train_val_idx, test_idx in strat_group_kfold.split(X, y, groups=X['MatchId']):\n",
    "        X_train_val, X_test = X.iloc[train_val_idx], X.iloc[test_idx]\n",
    "        y_train_val, y_test = y.iloc[train_val_idx], y.iloc[test_idx]\n",
    "        break  # Only take the first split (train/val vs test)\n",
    "\n",
    "    # Now, `X_train_val` and `X_test` are stratified by `y`, with the same `MatchId` grouping\n",
    "    # Same again for Splitting Training and Validation\n",
    "    strat_group_kfold_inner = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # This will give stratified splits while keeping `MatchId` groups together\n",
    "    # We only want to use the first split (train/val, test split)\n",
    "    for train_idx, val_idx in strat_group_kfold_inner.split(X_train_val, y_train_val, groups=X_train_val['MatchId']):\n",
    "        X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "        y_train, y_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n",
    "        break  # Only take the first split (train/val vs test)\n",
    "\n",
    "    # If instructed only keep the predictor columns\n",
    "    if remove_matchid:\n",
    "        X_train = X_train[cols]\n",
    "        X_val = X_val[cols]\n",
    "        X_test = X_test[cols]\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Split the data into overall training, validation and test set\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data_train_val_test(\n",
    "    X = events_processed[[\"MatchId\"] + cols_kpis_final_no_corr], \n",
    "    y = events_processed[\"epv_success\"], \n",
    "    random_state=random_state, \n",
    "    n_splits=5, \n",
    "    cols= cols_kpis_final_no_corr, \n",
    "    remove_matchid=False)\n",
    "\n",
    "# Prepare splits\n",
    "splits = {\n",
    "    \"Train\": (y_train, X_train),\n",
    "    \"Validation\": (y_val, X_val),\n",
    "    \"Test\": (y_test, X_test),\n",
    "    \"Overall\": (events_processed[\"epv_success\"], events_processed[[\"MatchId\"] + cols_kpis_final_no_corr])\n",
    "}\n",
    "\n",
    "# Construct summary table\n",
    "summary_data = {\n",
    "    \"Total Samples\": [len(y) for y, _ in splits.values()],\n",
    "    \"% Positive (y=1)\": [100 * sum(y) / len(y) for y, _ in splits.values()],\n",
    "    \"Distinct MatchIds\": [df[\"MatchId\"].nunique() for _, df in splits.values()]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data, index=splits.keys()).round(2)\n",
    "\n",
    "# Display table\n",
    "display(summary_df)\n",
    "\n",
    "# Define Class weight for the positive class as (# of negative samples / # of positive samples) in the training set\n",
    "class_weight = (len(y_train) - sum(y_train) )/ sum(y_train)\n",
    "print(\"Class weight of positive class: \", round(class_weight,1))\n",
    "\n",
    "# Check if there are highly correlating features in the training set\n",
    "print_high_correlations(X_train[cols_kpis_final_no_corr], threshold=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa764904",
   "metadata": {},
   "source": [
    "The table above shows that while grouping the samples w.r.t. Match IDs, the splitting does not achieve the exact same stratification through all sets; the test set contains proportionally less goals compared to the training and validation set. This is to be considered when evaluating models on the test set. The class weight for samples of the positive (goal) class are calculated based on the training set as number of negative samples divided by number of positive samples:\n",
    "\n",
    "\\begin{equation*}\\verb|class_weight| = \\frac{N_-^{\\text{train}}}{N_+^{\\text{train}}} \\approx 21.8\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80bbc37",
   "metadata": {},
   "source": [
    "For later model evaluatuions, define functions to evaluate and display model performance on overall test and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "68ec55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_on_testset(models: dict, features, add_info={}):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models on the test set and print results in a table.\n",
    "    \n",
    "    Parameters:\n",
    "    - models: dict, with model names as keys and fitted model objects as values\n",
    "    - features: list of feature names to use for prediction\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test[features])\n",
    "        y_pred_prob = model.predict_proba(X_test[features])[:, 1]\n",
    "\n",
    "        accuracy = round(accuracy_score(y_test, y_pred), 2)\n",
    "        recall = round(recall_score(y_test, y_pred), 2)\n",
    "        precision = round(precision_score(y_test, y_pred), 2)\n",
    "        f1 = round(f1_score(y_test, y_pred), 2)\n",
    "        logloss = round(log_loss(y_test, y_pred_prob), 2)\n",
    "        roc_auc = round(roc_auc_score(y_test, y_pred_prob), 2)\n",
    "        true_goals = int(sum(y_test))\n",
    "        predicted_goals = int(round(sum(y_pred_prob)))\n",
    "\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': accuracy,\n",
    "            'Recall': recall,\n",
    "            'Precision': precision,\n",
    "            'F1': f1,\n",
    "            'Log Loss': logloss,\n",
    "            'ROC AUC': roc_auc,\n",
    "            'Goals (True)': true_goals,\n",
    "            'Goals (Pred)': predicted_goals\n",
    "        })\n",
    "    \n",
    "    if len(add_info.keys()) > 0:\n",
    "        for key in add_info.keys():\n",
    "            results.append({\n",
    "                'Model': add_info[key]['Model'],\n",
    "                'Accuracy': add_info[key]['Accuracy'],\n",
    "                'Recall': add_info[key]['Recall'],\n",
    "                'Precision': add_info[key]['Precision'],\n",
    "                'F1': add_info[key]['F1'],\n",
    "                'Log Loss': add_info[key]['Log Loss'],\n",
    "                'ROC AUC': add_info[key]['ROC AUC'],\n",
    "                'Goals (True)': add_info[key]['Goals (True)'],\n",
    "                'Goals (Pred)': add_info[key]['Goals (Pred)']\n",
    "            })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    display(Markdown(\"### Evaluation Results on Test Set\"))\n",
    "    display(df_results)\n",
    "\n",
    "def evaluate_models_on_trainingset(models: dict, features, add_info = {}):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models on the train set and print results in a table.\n",
    "    \n",
    "    Parameters:\n",
    "    - models: dict, with model names as keys and fitted model objects as values\n",
    "    - features: list of feature names to use for prediction\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_train[features])\n",
    "        y_pred_prob = model.predict_proba(X_train[features])[:, 1]\n",
    "\n",
    "        accuracy = round(accuracy_score(y_train, y_pred), 2)\n",
    "        recall = round(recall_score(y_train, y_pred), 2)\n",
    "        precision = round(precision_score(y_train, y_pred), 2)\n",
    "        f1 = round(f1_score(y_train, y_pred), 2)\n",
    "        logloss = round(log_loss(y_train, y_pred_prob), 2)\n",
    "        roc_auc = round(roc_auc_score(y_train, y_pred_prob), 2)\n",
    "        true_goals = int(sum(y_train))\n",
    "        predicted_goals = int(round(sum(y_pred_prob)))\n",
    "\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': accuracy,\n",
    "            'Recall': recall,\n",
    "            'Precision': precision,\n",
    "            'F1': f1,\n",
    "            'Log Loss': logloss,\n",
    "            'ROC AUC': roc_auc,\n",
    "            'Goals (True)': true_goals,\n",
    "            'Goals (Pred)': predicted_goals\n",
    "        })\n",
    "    \n",
    "    if len(add_info.keys()) > 0:\n",
    "        for key in add_info.keys():\n",
    "            results.append({\n",
    "                'Model': add_info[key]['Model'],\n",
    "                'Accuracy': add_info[key]['Accuracy'],\n",
    "                'Recall': add_info[key]['Recall'],\n",
    "                'Precision': add_info[key]['Precision'],\n",
    "                'F1': add_info[key]['F1'],\n",
    "                'Log Loss': add_info[key]['Log Loss'],\n",
    "                'ROC AUC': add_info[key]['ROC AUC'],\n",
    "                'Goals (True)': add_info[key]['Goals (True)'],\n",
    "                'Goals (Pred)': add_info[key]['Goals (Pred)']\n",
    "            })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    display(Markdown(\"### Evaluation Results on Training Set\"))\n",
    "    display(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc450fa",
   "metadata": {},
   "source": [
    "## 2. Model Training using all Features ##\n",
    "\n",
    "In the following, using hyperparameter search with cross-validation, models are trained using all features (after removing high correlating ones).\n",
    "The model architectures used are Random Forest, XGBoost and Multilayer-Perceptrons (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bfda43",
   "metadata": {},
   "source": [
    "First, define two scoring metrics to select the best performing model: Binary cross-entropy with and without the use of class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d122cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scoring metric for hyperparameter search\n",
    "def weighed_bce_scorer(estimator, X, y):\n",
    "    \"\"\"\n",
    "    Custom weighted log loss: higher penalty for errors on the positive class.\n",
    "    \"\"\"\n",
    "    y_pred_proba = estimator.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Clip predictions to avoid log(0)\n",
    "    eps = 1e-15\n",
    "    y_pred_proba = np.clip(y_pred_proba, eps, 1 - eps)\n",
    "    \n",
    "    # Compute log loss manually with class weighting\n",
    "    loss = - (\n",
    "        class_weight * y * np.log(y_pred_proba) + \n",
    "        (1 - y) * np.log(1 - y_pred_proba)\n",
    "    )\n",
    "    return -np.mean(loss) \n",
    "\n",
    "# To inspect the influence of class weights, provide a second scorer that does not use class weights, i.e. the negative log loss\n",
    "def bce_scorer(estimator, X, y):\n",
    "    \"\"\"\n",
    "    standard log loss (negative to be)\n",
    "    \"\"\"\n",
    "    y_pred_proba = estimator.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Clip predictions to avoid log(0)\n",
    "    eps = 1e-15\n",
    "    y_pred_proba = np.clip(y_pred_proba, eps, 1 - eps)\n",
    "    \n",
    "    # Compute log loss manually with class weighting\n",
    "    loss = - (\n",
    "        y * np.log(y_pred_proba) + \n",
    "        (1 - y) * np.log(1 - y_pred_proba)\n",
    "    )\n",
    "    return -np.mean(loss) \n",
    "\n",
    "# Same scoring metrics, but with pytorch tensors as input\n",
    "def weighed_bce_scorer_probas(y_pred_proba,y):\n",
    "    # Clip predictions to avoid log(0)\n",
    "    eps = 1e-15\n",
    "    y_pred_proba = torch.clamp(y_pred_proba, eps, 1 - eps)\n",
    "    \n",
    "    loss = - (class_weight * y * torch.log(y_pred_proba) +\n",
    "              (1 - y) * torch.log(1 - y_pred_proba))\n",
    "    \n",
    "    return -torch.mean(loss).item()\n",
    "\n",
    "def bce_scorer_probas(y_pred_proba,y):\n",
    "    # Clip predictions to avoid log(0)\n",
    "    eps = 1e-15\n",
    "    y_pred_proba = torch.clamp(y_pred_proba, eps, 1 - eps)\n",
    "    \n",
    "    loss = - (y * torch.log(y_pred_proba) +\n",
    "              (1 - y) * torch.log(1 - y_pred_proba))\n",
    "    return -torch.mean(loss).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a4656",
   "metadata": {},
   "source": [
    "### 2.1.: Random Forest ###\n",
    "\n",
    "Perform two Hyperparameter Searches: One using the class-weighed binary cross-entropy loss as scoring metric and one with ordinary binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53226d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Search using class-weighed binary cross-entropy as scorer\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Average score of best parameters over 5 folds:  -0.16157980904974734\n",
      "Standard deviation of score of best parameters over 5 folds:  0.007969608153623345\n",
      "Hyperparameter Search using binary cross-entropy as scorer\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Average score of best parameters over 5 folds:  -1.1947929631758398\n",
      "Standard deviation of score of best parameters over 5 folds:  0.08736955601286074\n"
     ]
    }
   ],
   "source": [
    "# #############################\n",
    "# #############################\n",
    "# # RANDOM FORREST CLASSIFIER #\n",
    "# #############################\n",
    "# #############################\n",
    "\n",
    "def randomized_search_cv_random_forrest(X_train, y_train, scorer, n_folds, n_iter, random_state):\n",
    "    '''\n",
    "    Performs hyperparameter search with k-fold cross-validation on a given overall training set.\n",
    "    Returns the best hyperparameter combination.\n",
    "    '''\n",
    "    \n",
    "    # Define the parameter grid\n",
    "    param_dist = {\n",
    "        \"max_features\": [\"sqrt\",\"log2\"],  \n",
    "        # determines amount of features considered for each split\n",
    "        \"max_depth\": randint(6, 12),  \n",
    "        # depth limit for every single tree\n",
    "        \"min_samples_split\": randint(2, 6),  \n",
    "        # determines the minimum number of training samples in a node for a split in that node to be considered\n",
    "        \"min_samples_leaf\": randint(2, 6),  \n",
    "        # determines the minimum number of training samples in each leaf node of a given split \n",
    "        # (if the number is not met, the split is reverted)\n",
    "        \"class_weight\": [\"balanced\", None], \n",
    "        # determines wether class-weights are used in the splitting criterion of nodes\n",
    "        \"n_estimators\": randint(20, 200),  \n",
    "        # number of trees in the random forest\n",
    "        \"bootstrap\": [True],  \n",
    "        # method used to sample a subset of data points for the training of each tree\n",
    "        \"criterion\": [\"gini\",\"entropy\"], \n",
    "        # function to measure the split quality during tree construction\n",
    "        \"max_samples\": [0.3,0.4,0.5,0.6,0.7,1.0] \n",
    "        # determines what portion of the training set is subsampled for each tree\n",
    "    }\n",
    "\n",
    "    # groups for stratified group k fold\n",
    "    groups = X_train['MatchId']\n",
    "\n",
    "    # remove MatchId\n",
    "    X_train = X_train.drop(columns=['MatchId'])\n",
    "\n",
    "    # Stratified folds for cross-validation, grouped by match id\n",
    "    fold = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    rf_clf = RandomForestClassifier(random_state=random_state)\n",
    "    # Perform Randomized Search\n",
    "    clf_search = RandomizedSearchCV(\n",
    "        estimator=rf_clf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        scoring=scorer,#scorer,#make_scorer(roc_auc_score, needs_proba=True),\n",
    "        cv=fold,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        random_state=random_state,\n",
    "        error_score='raise'  # <--- change this temporarily\n",
    "    )\n",
    "    clf_search.fit(X_train, y_train, groups=groups)\n",
    "\n",
    "    # Store results of cross-validation in a dataframe\n",
    "    results_df = pd.DataFrame(clf_search.cv_results_)\n",
    "    # Get results for the best scoring parameter combination\n",
    "    best_params = clf_search.best_params_\n",
    "    best_params_index = clf_search.best_index_\n",
    "    best_score_avg = results_df.loc[best_params_index,'mean_test_score']\n",
    "    best_score_std = results_df.loc[best_params_index,'std_test_score']\n",
    "\n",
    "    print(f\"Average score of best parameters over {n_folds} folds: \", best_score_avg)\n",
    "    print(f\"Standard deviation of score of best parameters over {n_folds} folds: \", best_score_std)\n",
    "    # Return the best performing parameter combination\n",
    "    return best_params\n",
    "\n",
    "# Perform randomized search with 5-fold cross-validation for random forrest model, \n",
    "# for cross-validation, all data but the test set is used\n",
    "X_train_cv = pd.concat([X_train, X_val], axis=0)\n",
    "y_train_cv = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "# Without class weights in scoring metric\n",
    "print(\"Hyperparameter Search using binary cross-entropy as scorer\")\n",
    "best_rf_params_no_class_weights = randomized_search_cv_random_forrest(\n",
    "    X_train= X_train_cv,\n",
    "    y_train=y_train_cv,\n",
    "    scorer='neg_log_loss',#weighed_bce_scorer or 'neg_log_loss' # weighed_bce_scorer uses class weights, neg_log_loss does not\n",
    "    n_folds=5,\n",
    "    n_iter=100,\n",
    "    random_state=random_state)\n",
    "\n",
    "# With class weights in scoring metric\n",
    "print(\"Hyperparameter Search using class-weighed binary cross-entropy as scorer\")\n",
    "best_rf_params_with_class_weights = randomized_search_cv_random_forrest(\n",
    "    X_train= X_train_cv,\n",
    "    y_train=y_train_cv,\n",
    "    scorer=weighed_bce_scorer,#weighed_bce_scorer or 'neg_log_loss' # weighed_bce_scorer uses class weights, neg_log_loss does not\n",
    "    n_folds=5,\n",
    "    n_iter=100,\n",
    "    random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "17ba4cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Model Hyperparameters"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random Forest (no class weight scoring)</th>\n",
       "      <th>Random Forest (class weight scoring)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bootstrap</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_weight</th>\n",
       "      <td>None</td>\n",
       "      <td>balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>criterion</th>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_features</th>\n",
       "      <td>sqrt</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_samples</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_samples_split</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_estimators</th>\n",
       "      <td>134</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Random Forest (no class weight scoring)  \\\n",
       "bootstrap                                            True   \n",
       "class_weight                                         None   \n",
       "criterion                                         entropy   \n",
       "max_depth                                               8   \n",
       "max_features                                         sqrt   \n",
       "max_samples                                           0.3   \n",
       "min_samples_leaf                                        5   \n",
       "min_samples_split                                       5   \n",
       "n_estimators                                          134   \n",
       "\n",
       "                  Random Forest (class weight scoring)  \n",
       "bootstrap                                         True  \n",
       "class_weight                                  balanced  \n",
       "criterion                                      entropy  \n",
       "max_depth                                            6  \n",
       "max_features                                      sqrt  \n",
       "max_samples                                        0.7  \n",
       "min_samples_leaf                                     2  \n",
       "min_samples_split                                    3  \n",
       "n_estimators                                       186  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayerd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Evaluation Results on Training Set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Goals (True)</th>\n",
       "      <th>Goals (Pred)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (no class weights)</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>479</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest with class weights</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.86</td>\n",
       "      <td>479</td>\n",
       "      <td>4205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Model  Accuracy  Recall  Precision    F1  \\\n",
       "0  Random Forest (no class weights)      0.96    0.00       0.00  0.00   \n",
       "1  Random Forest with class weights      0.83    0.63       0.15  0.24   \n",
       "\n",
       "   Log Loss  ROC AUC  Goals (True)  Goals (Pred)  \n",
       "0      0.13     0.93           479           475  \n",
       "1      0.51     0.86           479          4205  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayerd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Evaluation Results on Test Set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Goals (True)</th>\n",
       "      <th>Goals (Pred)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (no class weights)</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.73</td>\n",
       "      <td>97</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest with class weights</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.75</td>\n",
       "      <td>97</td>\n",
       "      <td>1343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Model  Accuracy  Recall  Precision    F1  \\\n",
       "0  Random Forest (no class weights)      0.97    0.00       0.00  0.00   \n",
       "1  Random Forest with class weights      0.82    0.47       0.07  0.13   \n",
       "\n",
       "   Log Loss  ROC AUC  Goals (True)  Goals (Pred)  \n",
       "0      0.12     0.73            97           153  \n",
       "1      0.51     0.75            97          1343  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop MatchId column if it exists\n",
    "X_train_rf_clean = X_train_cv.drop(columns=['MatchId'], errors='ignore')\n",
    "X_test_clean = X_test.drop(columns=['MatchId'], errors='ignore')\n",
    "\n",
    "# Train the Random Forest with the best parameters (with and without class weights used in the scoring)\n",
    "rf_clf = RandomForestClassifier(**best_rf_params_no_class_weights, random_state=random_state)\n",
    "rf_clf.fit(X_train_rf_clean, y_train_cv)\n",
    "\n",
    "rf_clf_cw = RandomForestClassifier(**best_rf_params_with_class_weights, random_state=random_state)\n",
    "rf_clf_cw.fit(X_train_rf_clean, y_train_cv)\n",
    "\n",
    "# Display the hyperparameters of the best models\n",
    "params_df = pd.DataFrame({\"Random Forest (no class weight scoring)\": best_rf_params_no_class_weights, \n",
    "             \"Random Forest (class weight scoring)\": best_rf_params_with_class_weights})\n",
    "\n",
    "display(Markdown(\"### Model Hyperparameters\"))\n",
    "display(params_df)\n",
    "\n",
    "# Display best models performance on the training set and the test set\n",
    "evaluate_models_on_trainingset({\"Random Forest (no class weights)\": rf_clf, \"Random Forest with class weights\": rf_clf_cw},cols_kpis_final_no_corr)\n",
    "evaluate_models_on_testset({\"Random Forest (no class weights)\": rf_clf, \"Random Forest with class weights\": rf_clf_cw},cols_kpis_final_no_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ef505",
   "metadata": {},
   "source": [
    "Interpretation: Using class weights in the scoring leads to a slightly improved ROC-AUC score on the test set. The resulting model clearly overestimates goal probabilities as it predicts 1312 goals on the test set in total, while only 97 goals are contained in the set. The recall of the model using class weights is therefore relatively high, while the overall accuracy is lower than for the model without class weights. Importantly, with the standard decision threshold of 0.5, the model not using class weights does not predict a single goal, making it useless for class label prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523921bc",
   "metadata": {},
   "source": [
    "### 2.2.: XGBoost ###\n",
    "\n",
    "For XGBoost, early stopping is implemented during the hyperparameter search with cross-validation. As this is not built in by the package $\\verb|RandomizedSearchCV|$, the cross-validation and data splitting is implemented manually. Again, we do two overall runs of hyperparameter search: One with a class weighed binary cross-entropy loss as scoring metric and one without class-weights in the scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Search using binary cross-entropy as scorer\n",
      "Training XGBoost with randomized search and cross-validation\n",
      "Number of parameter combinations:  200\n",
      "Number of folds:  5\n",
      "Total number of models:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayerd\\AppData\\Local\\Temp\\ipykernel_2224\\2570392352.py:123: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  rs_results = pd.concat([rs_results, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  -0.1637\n",
      "Standard Deviation of best combination over splits:  0.00978\n",
      "Hyperparameter Search using weighed binary cross-entropy as scorer\n",
      "Training XGBoost with randomized search and cross-validation\n",
      "Number of parameter combinations:  200\n",
      "Number of folds:  5\n",
      "Total number of models:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayerd\\AppData\\Local\\Temp\\ipykernel_2224\\2570392352.py:123: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  rs_results = pd.concat([rs_results, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  -1.1944\n",
      "Standard Deviation of best combination over splits:  0.05475\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "#############################\n",
    "#### XGBOOST CLASSIFIER #####\n",
    "#############################\n",
    "#############################\n",
    "\n",
    "\n",
    "def randomized_search_cv_xgboost(X_train, y_train, scorer, n_folds, n_iter, random_state):\n",
    "\n",
    "    if n_folds < 3:\n",
    "        raise ValueError(\"n_folds must be at least 3 (need training, validation and testset)\")\n",
    "\n",
    "    # Definiere Suchbereich\n",
    "    param_dist = {\n",
    "        'objective': ['binary:logistic'],                   \n",
    "        # Loss Function Objective (The loss uses regularization aswell as class_weights)\n",
    "        'eval_metric': ['logloss'],                             \n",
    "        # evaluation metric used on the validation set for early stopping  \n",
    "        'scale_pos_weight': [1, 10, 15, class_weight],      \n",
    "        # class-weight for positive class used in the loss funciton\n",
    "        'max_depth': randint(3, 20),                        \n",
    "        # depth limit for every single tree\n",
    "        'n_estimators': np.arange(30, 200, 50),  \n",
    "        # Maximum amount of trees / boosting rounds\n",
    "        'eta': [0.01,0.05,0.08,0.1,0.15,0.2,0.3,0.4],  \n",
    "        # Learning rate, i.e. weight of each tree in the total logit prediction\n",
    "        'gamma': [0.0,0.1,0.2,0.3],                                     \n",
    "        # Minimale Loss-improvement to justify a split in a newly built tree\n",
    "        'subsample': [0.5,0.6,0.7,0.8,0.9,1.0], \n",
    "        # Subsampling for regularization\n",
    "        'colsample_bytree': [0.3,0.4,0.5,0.6,0.7,1.0],  \n",
    "        # Determines amount of features sampled for the construction of each tree\n",
    "        'lambda': [0, 0.001, 0.01, 0.1, 1, 5],  \n",
    "        # L2-Regularization, penalizes model complexity (large trees or logits in the tree leafs)\n",
    "        'early_stopping_rounds': [10]  \n",
    "        # early stopping if evaluation metric does not improve for more than 10 rounds on validation set\n",
    "    }\n",
    "\n",
    "    # Create folds for cross-validation\n",
    "    fold = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Dataframe for cv_results (can be used for inspection of parameters)\n",
    "    columns = list(param_dist.keys()) + [\"custom_score_\" + str(k+1) for k in range(n_folds)] + [\"avg_logloss\", \"avg_accuracy\", \"avg_recall\", \"avg_precision\", \"avg_roc_auc\", \"avg_custom_score\"]\n",
    "    rs_results = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Ziehe Parameter-Kombinationen\n",
    "    param_samples = ParameterSampler(param_dist, n_iter=n_iter, random_state=random_state)\n",
    "    print(\"Training XGBoost with randomized search and cross-validation\")\n",
    "    print(\"Number of parameter combinations: \", n_iter)\n",
    "    print(\"Number of folds: \", n_folds)\n",
    "    print(\"Total number of models: \", n_iter * n_folds)\n",
    "\n",
    "    # Pro Parameter-Kombination: trainiere Modell √ºber alle folds (1 fold als validierung, 1 fold als test), nutze jeweils validation set f√ºr early stopping, test f√ºr evaluation\n",
    "    for i, params in enumerate(param_samples):\n",
    "        row = {**params}\n",
    "        avg_logloss = 0\n",
    "        avg_accuracy = 0\n",
    "        avg_recall = 0\n",
    "        avg_precision = 0\n",
    "        avg_roc_auc = 0\n",
    "        avg_custom_score = 0\n",
    "        for fold_idx, (train_val_idx, test_idx) in enumerate(fold.split(X_train, y_train, groups=X_train['MatchId'])):\n",
    "            \n",
    "            \n",
    "            X_tr_vl, X_tst = X_train.iloc[train_val_idx], X_train.iloc[test_idx]\n",
    "            y_tr_vl, y_tst = y_train.iloc[train_val_idx], y_train.iloc[test_idx]\n",
    "            X_tst = X_tst.drop(columns=['MatchId'])\n",
    "            # teile train_val in train und validation, nur ein mal\n",
    "            for j, (train_idx, val_idx) in enumerate(fold.split(X_tr_vl, y_tr_vl, groups=X_tr_vl['MatchId'])):\n",
    "                X_tr, X_vl = X_tr_vl.iloc[train_idx], X_tr_vl.iloc[val_idx]\n",
    "                y_tr, y_vl = y_tr_vl.iloc[train_idx], y_tr_vl.iloc[val_idx]\n",
    "                X_tr = X_tr.drop(columns=['MatchId'])\n",
    "                X_vl = X_vl.drop(columns=['MatchId'])\n",
    "                break\n",
    "            \n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_vl, y_vl)],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Compute log loss and area under ROC curve on test set\n",
    "            y_pred_prob = model.predict_proba(X_tst)[:, 1]\n",
    "            test_log_loss = log_loss(y_tst, y_pred_prob)\n",
    "            auc = roc_auc_score(y_tst, y_pred_prob)\n",
    "\n",
    "            # compute recall, precision, f1, accuracy\n",
    "            y_pred = model.predict(X_tst)\n",
    "            accuracy = accuracy_score(y_tst, y_pred)\n",
    "            if sum(y_pred) != 0:\n",
    "                recall = recall_score(y_tst, y_pred)\n",
    "                precision = precision_score(y_tst, y_pred)\n",
    "            else:\n",
    "                recall=0\n",
    "                precision=0\n",
    "            \n",
    "            # compute the chosen scoring metric\n",
    "            custom_score = scorer(model,X_tst,y_tst)\n",
    "\n",
    "            # keep track on the results of the current splits\n",
    "            row[\"custom_score_\" + str(fold_idx+1)] = custom_score\n",
    "            avg_recall += recall\n",
    "            avg_precision += precision\n",
    "            avg_accuracy += accuracy\n",
    "            avg_logloss += test_log_loss\n",
    "            avg_roc_auc += auc\n",
    "            avg_custom_score += custom_score\n",
    "\n",
    "        # compute average results over all splits for the given parameter combination\n",
    "        avg_recall /= n_folds\n",
    "        avg_precision /= n_folds\n",
    "        avg_accuracy /= n_folds\n",
    "        avg_logloss /= n_folds\n",
    "        avg_roc_auc /= n_folds\n",
    "        avg_custom_score /= n_folds\n",
    "        row[\"avg_logloss\"] = avg_logloss\n",
    "        row[\"avg_accuracy\"] = avg_accuracy\n",
    "        row[\"avg_recall\"] = avg_recall\n",
    "        row[\"avg_precision\"] = avg_precision\n",
    "        row[\"avg_roc_auc\"] = avg_roc_auc\n",
    "        row[\"avg_custom_score\"] = avg_custom_score\n",
    "        rs_results = pd.concat([rs_results, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "    best_row = rs_results.loc[rs_results[\"avg_custom_score\"].idxmax()]\n",
    "    best_params = {param: best_row[param] for param in param_dist.keys()}\n",
    "\n",
    "    # print best score and standard_deviation over folds\n",
    "    best_score = best_row['avg_custom_score']\n",
    "    std_score = np.std([best_row[\"custom_score_\" + str(i+1)] for i in range(n_folds)])\n",
    "\n",
    "    print(\"Best Score: \", round(best_score,4))\n",
    "    print(\"Standard Deviation of best combination over splits: \", round(std_score,5))\n",
    "\n",
    "\n",
    "    return best_params\n",
    "\n",
    "# Without class weights in scoring metric\n",
    "print(\"Hyperparameter Search using binary cross-entropy as scorer\")\n",
    "best_xgb_params_no_class_weights = randomized_search_cv_xgboost(\n",
    "    X_train=X_train_cv, \n",
    "    y_train=y_train_cv, \n",
    "    scorer=bce_scorer, \n",
    "    n_folds=5,\n",
    "    n_iter=200,\n",
    "    random_state=random_state)\n",
    "print(\"Best Parameters: \", best_xgb_params_no_class_weights)\n",
    "\n",
    "# With class weights\n",
    "print(\"Hyperparameter Search using weighed binary cross-entropy as scorer\")\n",
    "best_xgb_params_with_class_weights = randomized_search_cv_xgboost(\n",
    "    X_train=X_train_cv, \n",
    "    y_train=y_train_cv, \n",
    "    scorer=weighed_bce_scorer, \n",
    "    n_folds=5,\n",
    "    n_iter=200,\n",
    "    random_state=random_state)\n",
    "print(\"Best Parameters: \", best_xgb_params_with_class_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e0f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary:logistic', 'eval_metric': 'logloss', 'scale_pos_weight': 1, 'max_depth': 3, 'n_estimators': 180, 'eta': np.float64(0.08), 'gamma': np.float64(0.3), 'subsample': np.float64(0.8), 'colsample_bytree': np.float64(1.0), 'lambda': np.float64(1.0), 'early_stopping_rounds': 10}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'logloss', 'scale_pos_weight': 21.82045929018789, 'max_depth': 3, 'n_estimators': 180, 'eta': np.float64(0.01), 'gamma': np.float64(0.2), 'subsample': np.float64(0.7), 'colsample_bytree': np.float64(0.3), 'lambda': np.float64(0.0), 'early_stopping_rounds': 10}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Model Hyperparameters"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XGBoost (no class weight scoring)</th>\n",
       "      <th>XGBoost (class weight scoring)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>objective</th>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>binary:logistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_metric</th>\n",
       "      <td>logloss</td>\n",
       "      <td>logloss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale_pos_weight</th>\n",
       "      <td>1</td>\n",
       "      <td>21.820459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_estimators</th>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eta</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early_stopping_rounds</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      XGBoost (no class weight scoring)  \\\n",
       "objective                               binary:logistic   \n",
       "eval_metric                                     logloss   \n",
       "scale_pos_weight                                      1   \n",
       "max_depth                                             3   \n",
       "n_estimators                                        180   \n",
       "eta                                                0.08   \n",
       "gamma                                               0.3   \n",
       "subsample                                           0.8   \n",
       "colsample_bytree                                    1.0   \n",
       "lambda                                              1.0   \n",
       "early_stopping_rounds                                10   \n",
       "\n",
       "                      XGBoost (class weight scoring)  \n",
       "objective                            binary:logistic  \n",
       "eval_metric                                  logloss  \n",
       "scale_pos_weight                           21.820459  \n",
       "max_depth                                          3  \n",
       "n_estimators                                     180  \n",
       "eta                                             0.01  \n",
       "gamma                                            0.2  \n",
       "subsample                                        0.7  \n",
       "colsample_bytree                                 0.3  \n",
       "lambda                                           0.0  \n",
       "early_stopping_rounds                             10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Evaluation of best performing model:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Evaluation Results on Training Set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Goals (True)</th>\n",
       "      <th>Goals (Pred)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost (no class weights)</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.83</td>\n",
       "      <td>479</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost with class weights</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.85</td>\n",
       "      <td>479</td>\n",
       "      <td>4656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy  Recall  Precision    F1  Log Loss  \\\n",
       "0  XGBoost (no class weights)      0.96    0.00       1.00  0.00      0.15   \n",
       "1  XGBoost with class weights      0.79    0.72       0.13  0.23      0.57   \n",
       "\n",
       "   ROC AUC  Goals (True)  Goals (Pred)  \n",
       "0     0.83           479           479  \n",
       "1     0.85           479          4656  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Evaluation Results on Test Set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Goals (True)</th>\n",
       "      <th>Goals (Pred)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost (no class weights)</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.72</td>\n",
       "      <td>97</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost with class weights</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.74</td>\n",
       "      <td>97</td>\n",
       "      <td>1491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy  Recall  Precision    F1  Log Loss  \\\n",
       "0  XGBoost (no class weights)      0.97    0.00       0.00  0.00      0.12   \n",
       "1  XGBoost with class weights      0.77    0.54       0.06  0.11      0.57   \n",
       "\n",
       "   ROC AUC  Goals (True)  Goals (Pred)  \n",
       "0     0.72            97           151  \n",
       "1     0.74            97          1491  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop MatchId column if it exists\n",
    "X_train_xgb_clean = X_train.drop(columns=['MatchId'], errors='ignore')\n",
    "X_val_xgb_clean = X_val.drop(columns=['MatchId'], errors='ignore')\n",
    "X_test_clean = X_test.drop(columns=['MatchId'], errors='ignore')\n",
    "\n",
    "# print(best_xgb_params_no_class_weights)\n",
    "\n",
    "# print(best_xgb_params_with_class_weights)\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(**best_xgb_params_no_class_weights)\n",
    "xgb_clf.fit(\n",
    "                X_train_xgb_clean, y_train,\n",
    "                eval_set=[(X_val_xgb_clean, y_val)],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "xgb_clf_cw = xgb.XGBClassifier(**best_xgb_params_with_class_weights)\n",
    "xgb_clf_cw.fit(\n",
    "                X_train_xgb_clean, y_train,\n",
    "                eval_set=[(X_val_xgb_clean, y_val)],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "# Display the hyperparameters of the best models\n",
    "params_df = pd.DataFrame({\"XGBoost (no class weight scoring)\": best_xgb_params_no_class_weights, \n",
    "             \"XGBoost (class weight scoring)\": best_xgb_params_with_class_weights})\n",
    "\n",
    "display(Markdown(\"### Model Hyperparameters\"))\n",
    "display(params_df)\n",
    "\n",
    "# Display the best model's performance on overall training and test set\n",
    "display(Markdown(\"## Evaluation of best performing model:\"))\n",
    "evaluate_models_on_trainingset({\"XGBoost (no class weights)\": xgb_clf, \"XGBoost with class weights\": xgb_clf_cw},cols_kpis_final_no_corr)\n",
    "evaluate_models_on_testset({\"XGBoost (no class weights)\": xgb_clf, \"XGBoost with class weights\": xgb_clf_cw},cols_kpis_final_no_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a7e57",
   "metadata": {},
   "source": [
    "Interpretation: The results are similar to those of the Random Forest classifier: With class weights, the best scoring model is using class weights during training - as expected - and is overly confident in predicting goals. It achieves a good recall score at the cost of missclassifying many unsuccessful situations, thereby achieving a poor accuracy. Also similarly to random forest, using class weights leads to a slightly better ROC-AUC Score, both during training and on the test set, from which we interpret that it improves the ranking ability of an XGBoost model in the given setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d10e1f",
   "metadata": {},
   "source": [
    "### 2.3.: Neural Network: Multilayer-Perceptron (MLP) ###\n",
    "\n",
    "Since a neural network, specifically an MLP has a fundamentally different structure from the tree-based ensemble models Random Forest and XGBoost, the training includes some additional preparation:\n",
    "\n",
    "- Three possible network infrastructures are proposed, varying in amount of hidden layers and neurons per layer (i.e. model complexity). The choice of infrastructure is a hyperparameter.\n",
    "- The feature values of the training set are rescaled such that the mean of each feature is 0 and the standard deviation is 1 over the overall training set. The transformation is then applied to the validation and test set.   \n",
    "\n",
    "The following block defines a function for hyperparameter search with cross-validation and performs a parameter search using the scoring metric with and without class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f12b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP with randomized search and cross-validation\n",
      "Number of parameter combinations:  40\n",
      "Number of folds:  5\n",
      "Total number of models:  200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayerd\\AppData\\Local\\Temp\\ipykernel_2224\\3155848205.py:257: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  rs_results = pd.concat([rs_results, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  -1.1807\n",
      "Standard Deviation of best combination over splits:  0.06373\n",
      "Training MLP with randomized search and cross-validation\n",
      "Number of parameter combinations:  40\n",
      "Number of folds:  5\n",
      "Total number of models:  200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayerd\\AppData\\Local\\Temp\\ipykernel_2224\\3155848205.py:257: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  rs_results = pd.concat([rs_results, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  -0.1673\n",
      "Standard Deviation of best combination over splits:  0.00836\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "#############################\n",
    "#### NN (MLP) Classifier ####\n",
    "#############################\n",
    "#############################\n",
    "\n",
    "### Define three MLP infrastructures: Small, medium and large\n",
    "\n",
    "class MLP_small(nn.Module):\n",
    "    # Input -> 32 -> 16 -> 1 -> Sigmoid (2 hidden layers)\n",
    "    def __init__(self, input_size, dropout_rate=0.0):\n",
    "        super(MLP_small, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)    # Linear Layer with 32 Neurons\n",
    "        self.relu = nn.ReLU()                   # Nonlinear Activation function (Rectified linear Unit)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)# Define dropout rate for the layer\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()  # Apply binary classification from raw model output using the sigmoid function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class MLP_medium(nn.Module):\n",
    "    # Input -> 64 -> 32 -> 1 -> Sigmoid (2 hidden layers)\n",
    "    def __init__(self, input_size, dropout_rate=0.0):\n",
    "        super(MLP_medium, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()  # Apply binary classification from raw model output using the sigmoid function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class MLP_large(nn.Module):\n",
    "    # Input -> 64 -> 32 -> 16 -> 1 -> Sigmoid (2 hidden layers)\n",
    "    def __init__(self, input_size,  dropout_rate=0.0):\n",
    "        super(MLP_large, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()  # Apply binary classification from raw model output using the sigmoid function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Define function to perform hyperparameter search with cross-validation\n",
    "def randomized_search_cv_nn(X_train, y_train, scorer, n_folds, n_iter, random_state, verbose = False):\n",
    "    if n_folds < 3:\n",
    "        raise ValueError(\"n_folds must be at least 3 (need training, validation and testset)\")\n",
    "    \n",
    "    # Number of features\n",
    "    n_features = len(X_train.columns)-1\n",
    "\n",
    "    # Define search grid\n",
    "    param_dist = {\n",
    "        'architecture': ['small','medium','large'],\n",
    "        # Determines the amount of hidden layers and neurons per layer\n",
    "        'batch_size': [32,64],\n",
    "        # Samples per batch\n",
    "        'learning_rate': [0.0002,0.0005,0.001],\n",
    "        # learning rate used during backpropagation (optimization of the loss function)\n",
    "        'weight_decay': [0.005,0.01,0.05],\n",
    "        # Regularization parameter used in the loss function to penalize large weights between layers / neurons\n",
    "        'dropout': [0.1,0.2,0.3],\n",
    "        # Dropout rate that determines the proportion of \"dead\" neurons per hidden layer sampled for each batch and \n",
    "        'pos_weight': [1,class_weight],\n",
    "        # Class weight parameter used in the loss function\n",
    "        'epochs': [60,90,120],\n",
    "        # Number of epochs\n",
    "        'early_stopping': [True],\n",
    "        # Early Stopping\n",
    "        # 'criterion': ['BCELossLogits']\n",
    "        # Defines loss function (weighed bce with regularization) together with 'pos_weight' and 'weight_decay'\n",
    "    }\n",
    "\n",
    "    # create folds\n",
    "    fold = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Dataframe for results (used to monitor and debug the parameter search)\n",
    "    columns = list(param_dist.keys()) + [\"custom_score_\" + str(k+1) for k in range(n_folds)] + [\"avg_logloss\", \"avg_accuracy\", \"avg_recall\", \"avg_precision\", \"avg_roc_auc\", \"avg_custom_score\"]\n",
    "    rs_results = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Draw random parameter combinations\n",
    "    param_samples = ParameterSampler(param_dist, n_iter=n_iter, random_state=random_state)\n",
    "    print(\"Training MLP with randomized search and cross-validation\")\n",
    "    print(\"Number of parameter combinations: \", n_iter)\n",
    "    print(\"Number of folds: \", n_folds)\n",
    "    print(\"Total number of models: \", n_iter * n_folds)\n",
    "\n",
    "    # For each parameter combination: train model over all splits, each time using a different fold for validation (early stopping) and evaluation\n",
    "    for i, params in enumerate(param_samples):\n",
    "\n",
    "        # initialize model performance parameters for currnt parameter combination\n",
    "        row = {**params}\n",
    "        avg_logloss = 0\n",
    "        avg_accuracy = 0\n",
    "        avg_recall = 0\n",
    "        avg_precision = 0\n",
    "        avg_roc_auc = 0\n",
    "        avg_custom_score = 0\n",
    "\n",
    "        # Iterate through splits and train model on the current parameters\n",
    "        for fold_idx, (train_val_idx, test_idx) in enumerate(fold.split(X_train, y_train, groups=X_train['MatchId'])):\n",
    "\n",
    "            X_tr_vl, X_tst = X_train.iloc[train_val_idx], X_train.iloc[test_idx]\n",
    "            y_tr_vl, y_tst = y_train.iloc[train_val_idx], y_train.iloc[test_idx]\n",
    "            X_tst = X_tst.drop(columns=['MatchId'])\n",
    "\n",
    "            # separate validation fold off the training folds of the current split\n",
    "            for j, (train_idx, val_idx) in enumerate(fold.split(X_tr_vl, y_tr_vl, groups=X_tr_vl['MatchId'])):\n",
    "                X_tr, X_vl = X_tr_vl.iloc[train_idx], X_tr_vl.iloc[val_idx]\n",
    "                y_tr, y_vl = y_tr_vl.iloc[train_idx], y_tr_vl.iloc[val_idx]\n",
    "                X_tr = X_tr.drop(columns=['MatchId'])\n",
    "                X_vl = X_vl.drop(columns=['MatchId'])\n",
    "                break\n",
    "\n",
    "            # Convert Dataframes to PyTorch tensors, normalize features on the training set of the current split\n",
    "            scaler = StandardScaler()\n",
    "            X_tr_tensor = torch.tensor(scaler.fit_transform(X_tr), dtype=torch.float32)\n",
    "            y_tr_tensor = torch.tensor(y_tr.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "            X_vl_tensor = torch.tensor(scaler.fit_transform(X_vl), dtype=torch.float32)\n",
    "            y_vl_tensor = torch.tensor(y_vl.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "            X_tst_tensor = torch.tensor(scaler.fit_transform(X_tst), dtype=torch.float32)\n",
    "            y_tst_tensor = torch.tensor(y_tst.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "            # Create DataLoader for batching during optimization\n",
    "            train_data = TensorDataset(X_tr_tensor, y_tr_tensor)\n",
    "            train_loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)\n",
    "\n",
    "            # Map hyperparameters architecture and dropout to the previously defined MLP architecture classes \n",
    "            if params['architecture'] == 'small':\n",
    "                model = MLP_small(input_size=n_features, dropout_rate=params['dropout'])\n",
    "            elif params['architecture'] == 'medium':\n",
    "                model = MLP_medium(input_size=n_features, dropout_rate=params['dropout'])\n",
    "            else:\n",
    "                model = MLP_large(input_size=n_features, dropout_rate=params['dropout'])\n",
    "\n",
    "            # define loss (weighed binary cross-entropy loss)\n",
    "            def custom_bce_loss(y_pred_prob, y_true):\n",
    "                w1 = params['pos_weight']   # class weight\n",
    "                w0 = 1 #/ (1 + w1)\n",
    "                w1 = w1 #/ (1 + w1)\n",
    "                crit = nn.BCELoss(reduction='none')\n",
    "                ls = crit(y_pred_prob, y_true)\n",
    "                return (ls * (y_true * w1 + (1 - y_true) * w0)).mean()\n",
    "            criterion = custom_bce_loss\n",
    "\n",
    "            # Adam Optimizer, including learning rate used during optimization and weight decay used in the loss function\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "            \n",
    "            # Define number of epochs (amount of times the training set is seen by the model)\n",
    "            num_epochs = params['epochs']\n",
    "\n",
    "            # Initialize loss history on validation set (for early stopping)\n",
    "            validation_loss = []\n",
    "\n",
    "            # Train model            \n",
    "            for epoch in range(num_epochs):\n",
    "                # Iterate through batches of the training set\n",
    "                for inputs, labels in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    # calculate loss and optimize weights of the MLP for the current batch\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # evaluate model output and loss on validation set for the current epoch\n",
    "                with torch.no_grad():\n",
    "                    y_vl_pred = model(X_vl_tensor)\n",
    "                    y_tr_pred = model(X_tr_tensor)\n",
    "                train_auc = roc_auc_score(y_tr_tensor, y_tr_pred)\n",
    "                train_loss = criterion(y_tr_pred, y_tr_tensor)\n",
    "                val_auc = roc_auc_score(y_vl_tensor, y_vl_pred)\n",
    "                val_loss = criterion(y_vl_pred, y_vl_tensor)\n",
    "                validation_loss.append(val_loss)\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f\"Epoch [{epoch + 1}/{num_epochs}], auc on training set: {train_auc:.2f}, Loss: {train_loss:.6f}, auc on validation set: {val_auc:.2f}, validation loss: {val_loss:.6f}\")\n",
    "                # If early stopping is performed and the validation loss has not improved for the last 5 epochs, stop the model training\n",
    "                if params['early_stopping']:\n",
    "                    if len(validation_loss) > 5 and validation_loss[-6] <= min(validation_loss[-5:]):\n",
    "                        if verbose:\n",
    "                            print(\"Early Stopping done after \", epoch, \" epochs\")\n",
    "                        break\n",
    "            \n",
    "            # evaluate model on the test fold\n",
    "            model.eval() # put model in evaluation mode (no dropout used)\n",
    "            with torch.no_grad():\n",
    "                y_tst_pred_prob = model(X_tst_tensor)\n",
    "                y_tst_pred = (y_tst_pred_prob > 0.5).float()  # Convert probabilities to 0/1\n",
    "\n",
    "            # compute and store the scoring metric for model selection on the test fold, \n",
    "            # as well as other metrics for model evaluation (this can be used to reduce the parameter grid) \n",
    "            test_log_loss = log_loss(y_tst_tensor, y_tst_pred_prob)\n",
    "            auc = roc_auc_score(y_tst_tensor, y_tst_pred_prob)\n",
    "            if sum(y_tst_pred) > 0:\n",
    "                recall = recall_score(y_tst_tensor, y_tst_pred)\n",
    "                precision = precision_score(y_tst_tensor, y_tst_pred)\n",
    "            else:\n",
    "                recall = 0\n",
    "                precision = 0\n",
    "            custom_score = scorer(y_tst_pred_prob,y_tst_tensor)\n",
    "            accuracy = accuracy_score(y_tst_tensor, y_tst_pred)\n",
    "            row[\"custom_score_\" + str(fold_idx+1)] = custom_score\n",
    "            avg_recall += recall\n",
    "            avg_precision += precision\n",
    "            avg_accuracy += accuracy\n",
    "            avg_logloss += test_log_loss\n",
    "            avg_roc_auc += auc\n",
    "            avg_custom_score += custom_score\n",
    "\n",
    "        # calculate average of model performance metrics for the given parameter combination over all splits\n",
    "        avg_recall /= n_folds\n",
    "        avg_precision /= n_folds\n",
    "        avg_accuracy /= n_folds\n",
    "        avg_logloss /= n_folds\n",
    "        avg_roc_auc /= n_folds\n",
    "        avg_custom_score /= n_folds\n",
    "        if len(rs_results) > 0:\n",
    "            if avg_custom_score > max(rs_results[\"avg_custom_score\"]) and verbose:\n",
    "                print(\"New best avg custom score: \", f\"{avg_custom_score:.4f}, avg auc: { avg_roc_auc:.4f}, avg logloss: {avg_logloss:.4f}, avg recall: {avg_recall:.4f}, avg precision: {avg_precision:.4f}, avg accuracy: {avg_accuracy:.4f}\")\n",
    "                print(\"Parameters: \", params)\n",
    "        row[\"avg_logloss\"] = avg_logloss\n",
    "        row[\"avg_accuracy\"] = avg_accuracy\n",
    "        row[\"avg_recall\"] = avg_recall\n",
    "        row[\"avg_precision\"] = avg_precision\n",
    "        row[\"avg_roc_auc\"] = avg_roc_auc\n",
    "        row[\"avg_custom_score\"] = avg_custom_score\n",
    "        rs_results = pd.concat([rs_results, pd.DataFrame([row])], ignore_index=True)\n",
    "    \n",
    "    # Identify Parameter combination with the best averag scoring metric result\n",
    "    best_row = rs_results.loc[rs_results[\"avg_custom_score\"].idxmax()]\n",
    "    best_params = {param: best_row[param] for param in param_dist.keys()}\n",
    "\n",
    "    # print best score and standard_deviation over folds\n",
    "    best_score = best_row['avg_custom_score']\n",
    "    std_score = np.std([best_row[\"custom_score_\" + str(i+1)] for i in range(n_folds)])\n",
    "\n",
    "    print(\"Best Score: \", round(best_score,4))\n",
    "    print(\"Standard Deviation of best combination over splits: \", round(std_score,5))\n",
    "    \n",
    "    # return best parameter combination\n",
    "    return best_params\n",
    "\n",
    "# Perform hyperparameter search using class weighed BCE scoring\n",
    "best_nn_params_with_class_weights = randomized_search_cv_nn(\n",
    "    X_train=X_train_cv,\n",
    "    y_train=y_train_cv,\n",
    "    scorer=weighed_bce_scorer_probas,\n",
    "    n_folds=5,\n",
    "    n_iter=40,\n",
    "    random_state=random_state,\n",
    "    verbose=False)\n",
    "\n",
    "# Perform hyperparameter search using ordinary BCE scoring\n",
    "best_nn_params_no_class_weights = randomized_search_cv_nn(\n",
    "    X_train=X_train_cv,\n",
    "    y_train=y_train_cv,\n",
    "    scorer=bce_scorer_probas,\n",
    "    n_folds=5,\n",
    "    n_iter=40,\n",
    "    random_state=random_state,\n",
    "    verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c90926",
   "metadata": {},
   "source": [
    "#### Use the best Hyperparameters (with and without the use of class-weights) to train the MLP on the overall training and validation set, evaluate it on the overall test set. ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e3b4933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (no class weight scoring): Early Stopping at epoch 23\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayerd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\mayerd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (with class weight scoring): Early Stopping at epoch 20\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Model Hyperparameters"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLP (no class weight scoring)</th>\n",
       "      <th>MLP (with class weight scoring)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>architecture</th>\n",
       "      <td>large</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_decay</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropout</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_weight</th>\n",
       "      <td>1.0</td>\n",
       "      <td>21.820459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epochs</th>\n",
       "      <td>120</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early_stopping</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               MLP (no class weight scoring) MLP (with class weight scoring)\n",
       "architecture                           large                           small\n",
       "batch_size                                64                              64\n",
       "learning_rate                         0.0005                          0.0005\n",
       "weight_decay                            0.01                            0.05\n",
       "dropout                                  0.3                             0.1\n",
       "pos_weight                               1.0                       21.820459\n",
       "epochs                                   120                              60\n",
       "early_stopping                          True                            True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Model Performance on training set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Goals (True)</th>\n",
       "      <th>Goals (Pred)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP (no class weight scoring)</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.78</td>\n",
       "      <td>479</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP (with class weight scoring)</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.79</td>\n",
       "      <td>479</td>\n",
       "      <td>4437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  Accuracy  Recall  Precision    F1  \\\n",
       "0    MLP (no class weight scoring)      0.96    0.00       0.00  0.00   \n",
       "1  MLP (with class weight scoring)      0.73    0.69       0.11  0.18   \n",
       "\n",
       "   Log Loss  ROC AUC  Goals (True)  Goals (Pred)  \n",
       "0      0.16     0.78           479           532  \n",
       "1      0.56     0.79           479          4437  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Model Performance on test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Goals (True)</th>\n",
       "      <th>Goals (Pred)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP (no class weight scoring)</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.72</td>\n",
       "      <td>97</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP (with class weight scoring)</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.73</td>\n",
       "      <td>97</td>\n",
       "      <td>1435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Model  Accuracy  Recall  Precision   F1  \\\n",
       "0    MLP (no class weight scoring)      0.97    0.00       0.00  0.0   \n",
       "1  MLP (with class weight scoring)      0.72    0.57       0.05  0.1   \n",
       "\n",
       "   Log Loss  ROC AUC  Goals (True)  Goals (Pred)  \n",
       "0      0.12     0.72            97           171  \n",
       "1      0.57     0.73            97          1435  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train MLPs with pest performing hyperparameters on the overall training and validation set\n",
    "best_params_models = {'MLP (no class weight scoring)': best_nn_params_no_class_weights,\n",
    "               'MLP (with class weight scoring)': best_nn_params_with_class_weights}\n",
    "\n",
    "# function to train and evaluate MLP with given parameters \n",
    "def train_and_evaluate_model(X_train_nn, y_train, X_val_nn, y_val, X_test_nn, y_test ,model_name, params, input_size, plotting=True):\n",
    "    '''trans an MLP with given parameters on the overall training and validation set,\n",
    "    evaluates model on the test set '''\n",
    "\n",
    "    # Convert Data to pytorch tensors and rescale feature values based on the training set\n",
    "    scaler = StandardScaler()\n",
    "    X_train_tensor = torch.tensor(scaler.fit_transform(X_train_nn), dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "    X_val_tensor = torch.tensor(scaler.transform(X_val_nn), dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "    X_test_tensor = torch.tensor(scaler.transform(X_test_nn), dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32).unsqueeze(1)   \n",
    "    \n",
    "    # Define architecture of MLP (hidden layers and neurons per layer)\n",
    "    if params['architecture'] == 'small':\n",
    "        model = MLP_small(input_size=input_size, dropout_rate=params['dropout'])\n",
    "    elif params['architecture'] == 'medium':\n",
    "        model = MLP_medium(input_size=input_size, dropout_rate=params['dropout'])\n",
    "    else:\n",
    "        model = MLP_large(input_size=input_size, dropout_rate=params['dropout'])\n",
    "\n",
    "    # define loss \n",
    "    def custom_bce_loss(y_pred_prob, y_true):\n",
    "            w1 = params['pos_weight']\n",
    "            w0 = 1 #/ (1 + w1)\n",
    "            w1 = w1 #/ (1 + w1)\n",
    "            crit = nn.BCELoss(reduction='none')\n",
    "            ls = crit(y_pred_prob, y_true)\n",
    "            return (ls * (y_true * w1 + (1 - y_true) * w0)).mean()\n",
    "    criterion = custom_bce_loss\n",
    "\n",
    "    # Adam optimizer, with learning rate and weight decay definition\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=params['batch_size'], shuffle=True)\n",
    "\n",
    "    # Training\n",
    "    validation_loss = []\n",
    "    training_loss = []\n",
    "    for epoch in range(params['epochs']):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Track training and validation loss\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_tensor)\n",
    "            val_loss = criterion(val_pred, y_val_tensor)\n",
    "            train_pred = model(X_train_tensor)\n",
    "            train_loss = criterion(train_pred, y_train_tensor)\n",
    "            training_loss.append(train_loss.item())\n",
    "            validation_loss.append(val_loss.item())\n",
    "            # print(\"Training loss\",round(train_loss.item(),4))\n",
    "            # print(\"Validation Loss\", round(val_loss.item(),4))\n",
    "            # y_train_pred_prob = model(X_train_tensor).squeeze()\n",
    "            # print(\"Train\", round(bce_scorer_probas(y_train_pred_prob,y_train_tensor),3), \" vs. \", round(log_loss(y_train, y_train_pred_prob.numpy()),3))\n",
    "            # y_test_pred_prob = model(X_test_tensor).squeeze()\n",
    "            # print(\"Test\",round(bce_scorer_probas(y_test_pred_prob,y_test_tensor),3), \" vs. \", round(log_loss(y_test_tensor, y_test_pred_prob.numpy()),3))\n",
    "\n",
    "            # Early stopping\n",
    "            early_stopping_rounds=10\n",
    "            if params['early_stopping']:\n",
    "                if len(validation_loss) > early_stopping_rounds and validation_loss[-early_stopping_rounds-1] <= min(validation_loss[-early_stopping_rounds:]):\n",
    "                    print(f\"{model_name}: Early Stopping at epoch {epoch}\")\n",
    "                    break\n",
    "    \n",
    "    if plotting:\n",
    "        # Plot training and validation loss\n",
    "        epochs = range(1, len(training_loss) + 1)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(epochs, training_loss, label='Training Loss', marker='o')\n",
    "        plt.plot(epochs, validation_loss, label='Validation Loss', marker='o')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Training vs. Validation Loss, {model_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()    # put model in evaluation mode (no dropout used)\n",
    "    with torch.no_grad():\n",
    "        # Test set predictions\n",
    "        y_test_pred_prob = model(X_test_tensor).squeeze()\n",
    "        y_test_pred = (y_test_pred_prob > 0.5).float()\n",
    "\n",
    "        # Train set predictions\n",
    "        y_train_pred_prob = model(X_train_tensor).squeeze()\n",
    "        y_train_pred = (y_train_pred_prob > 0.5).float()\n",
    "\n",
    "    # Test set metrics\n",
    "    test_metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': round(accuracy_score(y_test, y_test_pred), 2),\n",
    "        'Recall': round(recall_score(y_test, y_test_pred), 2),\n",
    "        'Precision': round(precision_score(y_test, y_test_pred), 2),\n",
    "        'F1': round(f1_score(y_test, y_test_pred), 2),\n",
    "        'Log Loss': round(log_loss(y_test, y_test_pred_prob.numpy()), 2),\n",
    "        'ROC AUC': round(roc_auc_score(y_test, y_test_pred_prob.numpy()), 2),\n",
    "        'Goals (True)': int(sum(y_test)),\n",
    "        'Goals (Pred)': int(round(sum(y_test_pred_prob.numpy())))\n",
    "    }\n",
    "\n",
    "    # Train set metrics\n",
    "    train_metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': round(accuracy_score(y_train, y_train_pred), 2),\n",
    "        'Recall': round(recall_score(y_train, y_train_pred), 2),\n",
    "        'Precision': round(precision_score(y_train, y_train_pred), 2),\n",
    "        'F1': round(f1_score(y_train, y_train_pred), 2),\n",
    "        'Log Loss': round(log_loss(y_train, y_train_pred_prob.numpy()), 2),\n",
    "        'ROC AUC': round(roc_auc_score(y_train, y_train_pred_prob.numpy()), 2),\n",
    "        'Goals (True)': int(sum(y_train)),\n",
    "        'Goals (Pred)': int(round(sum(y_train_pred_prob.numpy())))\n",
    "    }\n",
    "\n",
    "    return train_metrics, test_metrics, model\n",
    "\n",
    "# train best model (with and without class-weights for scoring)\n",
    "# Define overall training, validation and test set\n",
    "X_train_nn = X_train.drop(columns=['MatchId'], errors='ignore')\n",
    "X_test_nn = X_test.drop(columns=['MatchId'],errors='ignore')\n",
    "X_val_nn = X_val.drop(columns=['MatchId'],errors='ignore')\n",
    "train_results = []\n",
    "test_results = []\n",
    "best_clfs = []\n",
    "input_size = len(X_train_nn.columns)\n",
    "for model_name, params in best_params_models.items():\n",
    "    train_result, test_result, clf = train_and_evaluate_model(\n",
    "        X_train_nn=X_train_nn,\n",
    "        y_train=y_train,\n",
    "        X_val_nn=X_val_nn,\n",
    "        y_val=y_val,\n",
    "        X_test_nn=X_test_nn,\n",
    "        y_test=y_test,\n",
    "        model_name = model_name, \n",
    "        params = params, \n",
    "        input_size = input_size)\n",
    "    train_results.append(train_result)\n",
    "    test_results.append(test_result)\n",
    "    best_clfs.append(clf)\n",
    "\n",
    "mlp_clf, mlp_clf_cw = best_clfs[0], best_clfs[1] \n",
    "\n",
    "# set variables for the best models\n",
    "\n",
    "# Display the hyperparameters of the best models\n",
    "display(Markdown(\"### Model Hyperparameters\"))\n",
    "display(pd.DataFrame(best_params_models))\n",
    "\n",
    "# Display results of best models on training and test set\n",
    "df_results_train = pd.DataFrame(train_results)\n",
    "display(Markdown(\"### Model Performance on training set\"))\n",
    "display(df_results_train)\n",
    "\n",
    "df_results_test = pd.DataFrame(test_results)\n",
    "display(Markdown(\"### Model Performance on test set\"))\n",
    "display(df_results_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74a07f",
   "metadata": {},
   "source": [
    "## 3. Global Feature Importances ##\n",
    "\n",
    "Now we have a best model using all 32 correlation-filtered features. To reduce model complexity and to ease explainability of the final models, we select 15 overall top performing features.\n",
    "As the models using class weights perform better on the ROC AUC score, we continue with the best models using class-weights.\n",
    "To select 15 features, \n",
    "1. Calculate global feature importance (sum of impurity reduction for tree based models / absolute SHAP values on the test set for MLP, scaled so that all feature importances sum up to 1) for each architecture.\n",
    "2. Calculate average importance over all three architectures per feature, sort them by the average.\n",
    "3. From the 20 features with the highest average, manually select 15 features in consultation with match analysts at TSG Hoffenheim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13827ba",
   "metadata": {},
   "source": [
    "First, calculate global Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367db974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 3523it [00:42, 63.21it/s]                          \n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "###############################\n",
    "## Global Feature Importance ##\n",
    "###############################\n",
    "###############################\n",
    "\n",
    "# Tree based models Random Forest and XGBoost: use built in scaled impurity reduction per feature\n",
    "feature_names = X_test.columns\n",
    "ft_importance_rf = rf_clf_cw.feature_importances_\n",
    "ft_importance_xgb = xgb_clf_cw.feature_importances_\n",
    "\n",
    "\n",
    "# MLP: Use summed absolute SHAP values on the test set samples\n",
    "# Need to transform between numpy and torch data formats\n",
    "def model_wrapper(x):\n",
    "    x_tensor = torch.tensor(x, dtype=torch.float32)  # Convert NumPy array to tensor\n",
    "    with torch.no_grad():\n",
    "        return mlp_clf_cw(x_tensor).numpy()  # Convert output back to NumPy\n",
    "# Convert input tensors to NumPy (SHAP requires NumPy arrays)\n",
    "X_test_np = X_test_tensor.numpy()\n",
    "# Adjust column names\n",
    "explainer = shap.Explainer(model_wrapper, X_test_np, feature_names=feature_names)\n",
    "# Compute SHAP values\n",
    "shap_values = explainer(X_test_np)\n",
    "# SHAP values are in shap_values.values with shape (n_samples, n_features)\n",
    "shap_array = np.abs(shap_values.values)  # Take absolute values\n",
    "# Sum over samples (axis=0) to get total importance per feature\n",
    "shap_importance = shap_array.sum(axis=0)\n",
    "# Normalize by the total sum to get values adding up to 1\n",
    "ft_importance_mlp = shap_importance / shap_importance.sum()\n",
    "# Put importances per model in a dataframe and display them\n",
    "dict_model_names = {\n",
    "    \"Random Forest\": ft_importance_rf,\n",
    "    \"XGBoost\": ft_importance_xgb,\n",
    "    \"MLP\": ft_importance_mlp\n",
    "}\n",
    "dict_ft_imps_all = {}\n",
    "for name in dict_model_names.keys():\n",
    "    imps = dict_model_names[name]\n",
    "    dict_ft_imps = {feature_names[i]: imps[i] for i in range(len(feature_names))}\n",
    "    dict_ft_imps_all[name] = dict_ft_imps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf66a6",
   "metadata": {},
   "source": [
    "Calculate average importance over all three model architectures and display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "616ea019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Global Feature Importances (in %) over different model architectures"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>MLP</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sp_position_distance_to_goal</th>\n",
       "      <td>1.</td>\n",
       "      <td>20.8</td>\n",
       "      <td>11.4</td>\n",
       "      <td>18.9</td>\n",
       "      <td>17.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_position_vertical_zone</th>\n",
       "      <td>2.</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_first_ball_action_ball_z_max</th>\n",
       "      <td>3.</td>\n",
       "      <td>7.1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>8.3</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_heights_shot_zone_defense_mean</th>\n",
       "      <td>4.</td>\n",
       "      <td>7.4</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_heights_shot_zone_offense_count</th>\n",
       "      <td>5.</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>8.9</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_delivery_velocity</th>\n",
       "      <td>6.</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_space_control_in_box_offense</th>\n",
       "      <td>7.</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_delivery_distance</th>\n",
       "      <td>8.</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_position_angle_to_goal</th>\n",
       "      <td>9.</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_position_horizontal_zone</th>\n",
       "      <td>10.</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_delivery_end_position_angle_to_goal</th>\n",
       "      <td>11.</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_delivery_end_position_distance_to_goal</th>\n",
       "      <td>12.</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_delivery_end_position_vertical_zone</th>\n",
       "      <td>13.</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_avg_speeds_shot_zone_offense_mean</th>\n",
       "      <td>14.</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_avg_speeds_shot_zone_offense_min</th>\n",
       "      <td>15.</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_duration</th>\n",
       "      <td>16.</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_space_control_in_dfb_shot_zone_offense</th>\n",
       "      <td>17.</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_defensive_pressure_shot_zone_mean</th>\n",
       "      <td>18.</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_delivery_time</th>\n",
       "      <td>19.</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_heights_shot_zone_offense_min</th>\n",
       "      <td>20.</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_initial_last_line_of_defense</th>\n",
       "      <td>21.</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_numerical_superiority_in_dfb_shot_zone_offense</th>\n",
       "      <td>22.</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_heights_shot_zone_defense_max</th>\n",
       "      <td>23.</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_heights_shot_zone_offense_mean</th>\n",
       "      <td>24.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_heights_shot_zone_offense_max</th>\n",
       "      <td>25.</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_defensive_pressure_shot_zone_min</th>\n",
       "      <td>26.</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_pass_number</th>\n",
       "      <td>27.</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_heights_shot_zone_offense_taller_than_defense</th>\n",
       "      <td>28.</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_initial_inswing_outswing</th>\n",
       "      <td>29.</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_defenders_in_line_of_shot</th>\n",
       "      <td>30.</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_type</th>\n",
       "      <td>31.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_initial_marking_type</th>\n",
       "      <td>32.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp_early_dropping</th>\n",
       "      <td>33.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Position  Random Forest  \\\n",
       "Feature Name                                                                \n",
       "sp_position_distance_to_goal                            1.           20.8   \n",
       "sp_position_vertical_zone                               2.            6.2   \n",
       "sp_first_ball_action_ball_z_max                         3.            7.1   \n",
       "sp_heights_shot_zone_defense_mean                       4.            7.4   \n",
       "sp_heights_shot_zone_offense_count                      5.            3.6   \n",
       "sp_delivery_velocity                                    6.            4.8   \n",
       "sp_space_control_in_box_offense                         7.            3.5   \n",
       "sp_delivery_distance                                    8.            6.0   \n",
       "sp_position_angle_to_goal                               9.            3.7   \n",
       "sp_position_horizontal_zone                            10.            2.5   \n",
       "sp_delivery_end_position_angle_to_goal                 11.            2.3   \n",
       "sp_delivery_end_position_distance_to_goal              12.            3.9   \n",
       "sp_delivery_end_position_vertical_zone                 13.            0.9   \n",
       "sp_avg_speeds_shot_zone_offense_mean                   14.            1.6   \n",
       "sp_avg_speeds_shot_zone_offense_min                    15.            1.2   \n",
       "sp_duration                                            16.            3.1   \n",
       "sp_space_control_in_dfb_shot_zone_offense              17.            2.4   \n",
       "sp_defensive_pressure_shot_zone_mean                   18.            2.9   \n",
       "sp_delivery_time                                       19.            3.1   \n",
       "sp_heights_shot_zone_offense_min                       20.            1.4   \n",
       "sp_initial_last_line_of_defense                        21.            2.9   \n",
       "sp_numerical_superiority_in_dfb_shot_zone_offense      22.            1.2   \n",
       "sp_heights_shot_zone_defense_max                       23.            1.3   \n",
       "sp_heights_shot_zone_offense_mean                      24.            2.0   \n",
       "sp_heights_shot_zone_offense_max                       25.            1.3   \n",
       "sp_defensive_pressure_shot_zone_min                    26.            1.6   \n",
       "sp_pass_number                                         27.            0.4   \n",
       "sp_heights_shot_zone_offense_taller_than_defense       28.            0.4   \n",
       "sp_initial_inswing_outswing                            29.            0.2   \n",
       "sp_defenders_in_line_of_shot                           30.            0.1   \n",
       "sp_type                                                31.            0.0   \n",
       "sp_initial_marking_type                                32.            0.0   \n",
       "sp_early_dropping                                      33.            0.0   \n",
       "\n",
       "                                                   XGBoost   MLP  Average  \n",
       "Feature Name                                                               \n",
       "sp_position_distance_to_goal                          11.4  18.9     17.1  \n",
       "sp_position_vertical_zone                              6.1   9.0      7.1  \n",
       "sp_first_ball_action_ball_z_max                        4.6   8.3      6.7  \n",
       "sp_heights_shot_zone_defense_mean                      5.5   6.5      6.4  \n",
       "sp_heights_shot_zone_offense_count                     5.5   8.9      6.0  \n",
       "sp_delivery_velocity                                   3.0   4.3      4.0  \n",
       "sp_space_control_in_box_offense                        3.9   4.0      3.8  \n",
       "sp_delivery_distance                                   4.1   0.7      3.6  \n",
       "sp_position_angle_to_goal                              3.2   2.9      3.3  \n",
       "sp_position_horizontal_zone                            5.1   2.2      3.3  \n",
       "sp_delivery_end_position_angle_to_goal                 2.5   5.0      3.3  \n",
       "sp_delivery_end_position_distance_to_goal              3.4   2.1      3.2  \n",
       "sp_delivery_end_position_vertical_zone                 3.0   3.6      2.5  \n",
       "sp_avg_speeds_shot_zone_offense_mean                   1.9   3.8      2.4  \n",
       "sp_avg_speeds_shot_zone_offense_min                    2.7   3.0      2.3  \n",
       "sp_duration                                            3.6   0.2      2.3  \n",
       "sp_space_control_in_dfb_shot_zone_offense              3.2   1.2      2.3  \n",
       "sp_defensive_pressure_shot_zone_mean                   2.7   1.0      2.2  \n",
       "sp_delivery_time                                       3.3   0.3      2.2  \n",
       "sp_heights_shot_zone_offense_min                       2.7   2.2      2.1  \n",
       "sp_initial_last_line_of_defense                        2.5   1.0      2.1  \n",
       "sp_numerical_superiority_in_dfb_shot_zone_offense      2.2   2.0      1.8  \n",
       "sp_heights_shot_zone_defense_max                       2.4   1.6      1.8  \n",
       "sp_heights_shot_zone_offense_mean                      2.5   0.5      1.7  \n",
       "sp_heights_shot_zone_offense_max                       1.9   1.2      1.5  \n",
       "sp_defensive_pressure_shot_zone_min                    2.0   0.6      1.4  \n",
       "sp_pass_number                                         1.6   1.5      1.2  \n",
       "sp_heights_shot_zone_offense_taller_than_defense       1.9   1.3      1.2  \n",
       "sp_initial_inswing_outswing                            1.7   1.0      1.0  \n",
       "sp_defenders_in_line_of_shot                           0.0   1.1      0.4  \n",
       "sp_type                                                0.0   0.0      0.0  \n",
       "sp_initial_marking_type                                0.0   0.0      0.0  \n",
       "sp_early_dropping                                      0.0   0.0      0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Put importances in a dataframe\n",
    "df_ft_importances = pd.DataFrame(dict_ft_imps_all)\n",
    "# Compute average \n",
    "df_ft_importances[\"Average\"] = df_ft_importances[list(dict_model_names.keys())].mean(axis=1)\n",
    "# For displaying: Multiply all importances by 100 and round to 1 decimal place\n",
    "df_ft_importances = df_ft_importances.map(lambda x: round(x * 100, 1))\n",
    "# Sort by average importance (descending)\n",
    "df_ft_importances = df_ft_importances.sort_values(by=\"Average\", ascending=False)\n",
    "# Insert position as the first column\n",
    "df_ft_importances.insert(0, \"Position\", [f\"{i+1}.\" for i in range(len(df_ft_importances))])\n",
    "# Rename the index\n",
    "df_ft_importances.index.name = \"Feature Name\"\n",
    "display(Markdown('### Global Feature Importances (in %) over different model architectures'))\n",
    "display(df_ft_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494ce63e",
   "metadata": {},
   "source": [
    "On the basis of this analysis, the following 15 features have been selected in consultation with match analysts at TSG Hoffenheim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "19385aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_15_features = ['sp_position_distance_to_goal',\n",
    "                    'sp_position_vertical_zone',\n",
    "                    'sp_first_ball_action_ball_z_max',\n",
    "                    'sp_heights_shot_zone_defense_mean',\n",
    "                    'sp_heights_shot_zone_offense_count',\n",
    "                    'sp_delivery_velocity',\n",
    "                    'sp_space_control_in_box_offense',\n",
    "                    'sp_delivery_distance',\n",
    "                    'sp_position_angle_to_goal',\n",
    "                    'sp_delivery_end_position_angle_to_goal',\n",
    "                    'sp_delivery_end_position_distance_to_goal',\n",
    "                    'sp_defensive_pressure_shot_zone_mean',\n",
    "                    'sp_avg_speeds_shot_zone_offense_mean',\n",
    "                    'sp_duration',\n",
    "                    'sp_heights_shot_zone_offense_mean']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Plot feature importance\n",
    "# shap.summary_plot(shap_values, X_test_np)\n",
    "# plt.show()\n",
    "# sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee44d07",
   "metadata": {},
   "source": [
    "## 4. Model Training with reduced Features ##\n",
    "\n",
    "with the selected top 15 features, another hyperparameter search is performed for Random Forest, XGBoost and MLP.\n",
    "Based on the results of models on all features, we from now on only consider models using class weight during scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "ad7200cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Average score of best parameters over 5 folds:  -1.18050079594461\n",
      "Standard deviation of score of best parameters over 5 folds:  0.09047050035620532\n",
      "Training XGBoost with randomized search and cross-validation\n",
      "Number of parameter combinations:  100\n",
      "Number of folds:  5\n",
      "Total number of models:  500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayerd\\AppData\\Local\\Temp\\ipykernel_2224\\2570392352.py:123: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  rs_results = pd.concat([rs_results, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  -1.1921\n",
      "Standard Deviation of best combination over splits:  0.0533\n",
      "Training MLP with randomized search and cross-validation\n",
      "Number of parameter combinations:  40\n",
      "Number of folds:  5\n",
      "Total number of models:  200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayerd\\AppData\\Local\\Temp\\ipykernel_2224\\3155848205.py:257: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  rs_results = pd.concat([rs_results, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  -0.1656\n",
      "Standard Deviation of best combination over splits:  0.00719\n"
     ]
    }
   ],
   "source": [
    "# Define reduced Training sets only containing top 15 features\n",
    "X_train_top15 = X_train[[\"MatchId\"] + top_15_features]\n",
    "X_val_top15 = X_val[[\"MatchId\"] + top_15_features]\n",
    "X_train_cv_top15 = X_train_cv[[\"MatchId\"] + top_15_features]\n",
    "X_test_top15 = X_test[top_15_features]\n",
    "\n",
    "#\n",
    "best_rf_params_top15 = randomized_search_cv_random_forrest(\n",
    "    X_train=X_train_cv_top15,\n",
    "    y_train=y_train_cv,\n",
    "    scorer=weighed_bce_scorer,\n",
    "    n_folds=5,\n",
    "    n_iter=100,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "best_xgb_params_top15 = randomized_search_cv_xgboost(\n",
    "    X_train=X_train_cv_top15,\n",
    "    y_train=y_train_cv,\n",
    "    scorer=weighed_bce_scorer,\n",
    "    n_folds=5,\n",
    "    n_iter=100,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "best_nn_params_top15 = randomized_search_cv_nn(\n",
    "    X_train=X_train_cv_top15,\n",
    "    y_train=y_train_cv,\n",
    "    scorer=bce_scorer_probas,\n",
    "    n_folds=5,\n",
    "    n_iter=40,\n",
    "    random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b52d179",
   "metadata": {},
   "source": [
    "Train and evaluate the best model for each architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "5923868b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP: Early Stopping at epoch 17\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Model Performance using Class weights and the Top  15 Features"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Evaluation Results on Training Set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Goals (True)</th>\n",
       "      <th>Goals (Pred)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.85</td>\n",
       "      <td>479</td>\n",
       "      <td>4149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.83</td>\n",
       "      <td>479</td>\n",
       "      <td>4701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.79</td>\n",
       "      <td>479</td>\n",
       "      <td>4482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  Accuracy  Recall  Precision    F1  Log Loss  ROC AUC  \\\n",
       "0  RandomForest      0.84    0.61       0.15  0.24      0.50     0.85   \n",
       "1       XGBoost      0.73    0.77       0.12  0.20      0.57     0.83   \n",
       "2           MLP      0.73    0.68       0.10  0.18      0.56     0.79   \n",
       "\n",
       "   Goals (True)  Goals (Pred)  \n",
       "0           479          4149  \n",
       "1           479          4701  \n",
       "2           479          4482  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Evaluation Results on Test Set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Goals (True)</th>\n",
       "      <th>Goals (Pred)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.75</td>\n",
       "      <td>97</td>\n",
       "      <td>1321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.74</td>\n",
       "      <td>97</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.73</td>\n",
       "      <td>97</td>\n",
       "      <td>1448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  Accuracy  Recall  Precision    F1  Log Loss  ROC AUC  \\\n",
       "0  RandomForest      0.84    0.44       0.08  0.13      0.51     0.75   \n",
       "1       XGBoost      0.71    0.57       0.05  0.10      0.58     0.74   \n",
       "2           MLP      0.71    0.56       0.05  0.10      0.57     0.73   \n",
       "\n",
       "   Goals (True)  Goals (Pred)  \n",
       "0            97          1321  \n",
       "1            97          1506  \n",
       "2            97          1448  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_top15 = X_train_top15[top_15_features]\n",
    "X_val_top15 = X_val_top15[top_15_features]\n",
    "X_train_cv_top15 = X_train_cv_top15[top_15_features]\n",
    "X_test_top15 = X_test_top15[top_15_features]\n",
    "\n",
    "## Train Best Random forest with top 15 features\n",
    "rf_clf_top15 = RandomForestClassifier(**best_rf_params_top15, random_state=random_state)\n",
    "rf_clf_top15.fit(X_train_cv_top15, y_train_cv)\n",
    "\n",
    "## Train best XGBoost with top 15 Features\n",
    "xgb_clf_top15 = xgb.XGBClassifier(**best_xgb_params_top15)\n",
    "xgb_clf_top15.fit(\n",
    "                X_train_top15, y_train,\n",
    "                eval_set=[(X_val_top15, y_val)],\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "## Train best MLP with top 15 Features\n",
    "train_result, test_result, mlp_clf_top15 = train_and_evaluate_model(\n",
    "        X_train_nn=X_train_nn,\n",
    "        y_train=y_train,\n",
    "        X_val_nn=X_val_nn,\n",
    "        y_val=y_val,\n",
    "        X_test_nn=X_test_nn,\n",
    "        y_test=y_test,\n",
    "        model_name = \"MLP\", \n",
    "        params = params, \n",
    "        input_size = input_size,\n",
    "        plotting=False)\n",
    "\n",
    "## Evaluate all three models\n",
    "display(Markdown('## Model Performance using Class weights and the Top 15 Features'))\n",
    "\n",
    "evaluate_models_on_trainingset({\"RandomForest\": rf_clf_top15, \"XGBoost\":xgb_clf_top15}, top_15_features, add_info={\"MLP\": train_result})\n",
    "\n",
    "evaluate_models_on_testset({\"RandomForest\": rf_clf_top15, \"XGBoost\":xgb_clf_top15}, top_15_features, add_info={\"MLP\": test_result})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f573e08a",
   "metadata": {},
   "source": [
    "## 5. Model Interpretation ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e29aad4",
   "metadata": {},
   "source": [
    "Using the best models on the top 15 features, the shapley values are inspected on a single sample level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a921b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Printe Klassischen SHAP Plot, Shap Plot per feature und einen Force plot als beispiel mit den top 15 modellen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
